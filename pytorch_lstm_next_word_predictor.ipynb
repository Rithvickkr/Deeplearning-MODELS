{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rithvickkr/Deeplearning-MODELS/blob/main/pytorch_lstm_next_word_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqgSg9lkirRq",
        "outputId": "fb819084-180d-432d-a2e2-c1512cb23c30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "ae7dunqczT8Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "fmKQYI8Z1CpI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(userdata.get('hf_token'))"
      ],
      "metadata": {
        "id": "Dz9Kf9gHgC8q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'data/train/train.jsonl.zst', 'validation': 'data/validation/validation.jsonl.zst', 'test': 'data/test/test.jsonl.zst'}\n",
        "df = pd.read_json(\"hf://datasets/dlwh/wikitext_2_detokenized/\" + splits[\"train\"], lines=True)\n"
      ],
      "metadata": {
        "id": "thOXFXSMv_8K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df[\"text\"].tolist()\n"
      ],
      "metadata": {
        "id": "hJWT00-nlbY_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word for sentence in texts for word in sentence.split()]\n"
      ],
      "metadata": {
        "id": "3XMlUi3tlhZR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocab\n",
        "word_counts = Counter(tokens)\n",
        "vocab = {word: i+1 for i, (word, _) in enumerate(word_counts.items())}  # Start from index 1 (0 is padding)\n",
        "vocab[\"<PAD>\"] = 0  # Add padding token\n"
      ],
      "metadata": {
        "id": "G30GxEjgzcfY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "SOOEZ94P0dQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed85610-2a55-4d6e-8bd6-afdba4330157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149583"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts = [[vocab[word] for word in sentence.split() if word in vocab] for sentence in texts]"
      ],
      "metadata": {
        "id": "RefNavJe1Cva"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, encoded_texts, seq_length=5):\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        for text in encoded_texts:\n",
        "            for i in range(len(text) - seq_length):\n",
        "                self.inputs.append(text[i:i+seq_length])  # Input sequence\n",
        "                self.targets.append(text[i+seq_length])   # Next word (target)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        self.inputs = torch.tensor(self.inputs, dtype=torch.long)\n",
        "        self.targets = torch.tensor(self.targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]\n"
      ],
      "metadata": {
        "id": "80rIx4aq6ele"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 5  # Input sequence length\n",
        " # Reduce dataset to 1000 samples for testing\n",
        "dataloader = DataLoader(NextWordDataset(encoded_texts, seq_length), batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "V_aGJ0fy7swk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ Define LSTM Model\n",
        "class NextWordModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
        "        super(NextWordModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (h_n, _) = self.lstm(x)  # Get last hidden state\n",
        "        output = self.fc(h_n[-1])  # Fully connected layer\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "q2Z_fiVZ8GRo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "vocab_size = len(vocab)\n",
        "model = NextWordModel(vocab_size)"
      ],
      "metadata": {
        "id": "4bIcIRd088EN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Training Setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "dtPg5uRN9Cc7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print every 10 batches\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "hqZssF989X-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52ceeeb-68af-40e6-960b-4fe3374de64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/50782], Loss: 6.4307\n",
            "Epoch [1/10], Step [20/50782], Loss: 6.8397\n",
            "Epoch [1/10], Step [30/50782], Loss: 6.3567\n",
            "Epoch [1/10], Step [40/50782], Loss: 8.0544\n",
            "Epoch [1/10], Step [50/50782], Loss: 6.5673\n",
            "Epoch [1/10], Step [60/50782], Loss: 6.7529\n",
            "Epoch [1/10], Step [70/50782], Loss: 6.9953\n",
            "Epoch [1/10], Step [80/50782], Loss: 6.9647\n",
            "Epoch [1/10], Step [90/50782], Loss: 6.9387\n",
            "Epoch [1/10], Step [100/50782], Loss: 6.2349\n",
            "Epoch [1/10], Step [110/50782], Loss: 7.3900\n",
            "Epoch [1/10], Step [120/50782], Loss: 7.9856\n",
            "Epoch [1/10], Step [130/50782], Loss: 7.5673\n",
            "Epoch [1/10], Step [140/50782], Loss: 6.7242\n",
            "Epoch [1/10], Step [150/50782], Loss: 7.2163\n",
            "Epoch [1/10], Step [160/50782], Loss: 6.7395\n",
            "Epoch [1/10], Step [170/50782], Loss: 6.0334\n",
            "Epoch [1/10], Step [180/50782], Loss: 7.6042\n",
            "Epoch [1/10], Step [190/50782], Loss: 7.3243\n",
            "Epoch [1/10], Step [200/50782], Loss: 7.4722\n",
            "Epoch [1/10], Step [210/50782], Loss: 7.2350\n",
            "Epoch [1/10], Step [220/50782], Loss: 7.0545\n",
            "Epoch [1/10], Step [230/50782], Loss: 7.2943\n",
            "Epoch [1/10], Step [240/50782], Loss: 5.8080\n",
            "Epoch [1/10], Step [250/50782], Loss: 6.4844\n",
            "Epoch [1/10], Step [260/50782], Loss: 7.6181\n",
            "Epoch [1/10], Step [270/50782], Loss: 8.2042\n",
            "Epoch [1/10], Step [280/50782], Loss: 7.4932\n",
            "Epoch [1/10], Step [290/50782], Loss: 7.2703\n",
            "Epoch [1/10], Step [300/50782], Loss: 9.5289\n",
            "Epoch [1/10], Step [310/50782], Loss: 8.3352\n",
            "Epoch [1/10], Step [320/50782], Loss: 6.3082\n",
            "Epoch [1/10], Step [330/50782], Loss: 6.6183\n",
            "Epoch [1/10], Step [340/50782], Loss: 6.5456\n",
            "Epoch [1/10], Step [350/50782], Loss: 7.1571\n",
            "Epoch [1/10], Step [360/50782], Loss: 7.4315\n",
            "Epoch [1/10], Step [370/50782], Loss: 6.0098\n",
            "Epoch [1/10], Step [380/50782], Loss: 6.3725\n",
            "Epoch [1/10], Step [390/50782], Loss: 8.9998\n",
            "Epoch [1/10], Step [400/50782], Loss: 6.4791\n",
            "Epoch [1/10], Step [410/50782], Loss: 7.0991\n",
            "Epoch [1/10], Step [420/50782], Loss: 7.4937\n",
            "Epoch [1/10], Step [430/50782], Loss: 6.0916\n",
            "Epoch [1/10], Step [440/50782], Loss: 8.5847\n",
            "Epoch [1/10], Step [450/50782], Loss: 6.8846\n",
            "Epoch [1/10], Step [460/50782], Loss: 7.3115\n",
            "Epoch [1/10], Step [470/50782], Loss: 7.6961\n",
            "Epoch [1/10], Step [480/50782], Loss: 6.7204\n",
            "Epoch [1/10], Step [490/50782], Loss: 8.6354\n",
            "Epoch [1/10], Step [500/50782], Loss: 5.2926\n",
            "Epoch [1/10], Step [510/50782], Loss: 5.0826\n",
            "Epoch [1/10], Step [520/50782], Loss: 6.2707\n",
            "Epoch [1/10], Step [530/50782], Loss: 6.6810\n",
            "Epoch [1/10], Step [540/50782], Loss: 7.1578\n",
            "Epoch [1/10], Step [550/50782], Loss: 6.9670\n",
            "Epoch [1/10], Step [560/50782], Loss: 8.2887\n",
            "Epoch [1/10], Step [570/50782], Loss: 6.2039\n",
            "Epoch [1/10], Step [580/50782], Loss: 6.7684\n",
            "Epoch [1/10], Step [590/50782], Loss: 7.9149\n",
            "Epoch [1/10], Step [600/50782], Loss: 8.3852\n",
            "Epoch [1/10], Step [610/50782], Loss: 5.5254\n",
            "Epoch [1/10], Step [620/50782], Loss: 8.2391\n",
            "Epoch [1/10], Step [630/50782], Loss: 6.7738\n",
            "Epoch [1/10], Step [640/50782], Loss: 7.2242\n",
            "Epoch [1/10], Step [650/50782], Loss: 6.8723\n",
            "Epoch [1/10], Step [660/50782], Loss: 9.2993\n",
            "Epoch [1/10], Step [670/50782], Loss: 7.7746\n",
            "Epoch [1/10], Step [680/50782], Loss: 6.6551\n",
            "Epoch [1/10], Step [690/50782], Loss: 8.2410\n",
            "Epoch [1/10], Step [700/50782], Loss: 7.1015\n",
            "Epoch [1/10], Step [710/50782], Loss: 7.1713\n",
            "Epoch [1/10], Step [720/50782], Loss: 7.0303\n",
            "Epoch [1/10], Step [730/50782], Loss: 6.7638\n",
            "Epoch [1/10], Step [740/50782], Loss: 8.8178\n",
            "Epoch [1/10], Step [750/50782], Loss: 7.3022\n",
            "Epoch [1/10], Step [760/50782], Loss: 8.9508\n",
            "Epoch [1/10], Step [770/50782], Loss: 6.0921\n",
            "Epoch [1/10], Step [780/50782], Loss: 7.4408\n",
            "Epoch [1/10], Step [790/50782], Loss: 7.1850\n",
            "Epoch [1/10], Step [800/50782], Loss: 6.4165\n",
            "Epoch [1/10], Step [810/50782], Loss: 8.1741\n",
            "Epoch [1/10], Step [820/50782], Loss: 6.9370\n",
            "Epoch [1/10], Step [830/50782], Loss: 6.4432\n",
            "Epoch [1/10], Step [840/50782], Loss: 8.3154\n",
            "Epoch [1/10], Step [850/50782], Loss: 6.7531\n",
            "Epoch [1/10], Step [860/50782], Loss: 6.7577\n",
            "Epoch [1/10], Step [870/50782], Loss: 5.8933\n",
            "Epoch [1/10], Step [880/50782], Loss: 8.3070\n",
            "Epoch [1/10], Step [890/50782], Loss: 6.7132\n",
            "Epoch [1/10], Step [900/50782], Loss: 7.0929\n",
            "Epoch [1/10], Step [910/50782], Loss: 5.9211\n",
            "Epoch [1/10], Step [920/50782], Loss: 8.3173\n",
            "Epoch [1/10], Step [930/50782], Loss: 7.2877\n",
            "Epoch [1/10], Step [940/50782], Loss: 9.2546\n",
            "Epoch [1/10], Step [950/50782], Loss: 6.8342\n",
            "Epoch [1/10], Step [960/50782], Loss: 8.3936\n",
            "Epoch [1/10], Step [970/50782], Loss: 5.3860\n",
            "Epoch [1/10], Step [980/50782], Loss: 6.0215\n",
            "Epoch [1/10], Step [990/50782], Loss: 6.1992\n",
            "Epoch [1/10], Step [1000/50782], Loss: 8.9070\n",
            "Epoch [1/10], Step [1010/50782], Loss: 7.5920\n",
            "Epoch [1/10], Step [1020/50782], Loss: 7.8811\n",
            "Epoch [1/10], Step [1030/50782], Loss: 7.8876\n",
            "Epoch [1/10], Step [1040/50782], Loss: 7.8559\n",
            "Epoch [1/10], Step [1050/50782], Loss: 8.1179\n",
            "Epoch [1/10], Step [1060/50782], Loss: 6.2669\n",
            "Epoch [1/10], Step [1070/50782], Loss: 6.5796\n",
            "Epoch [1/10], Step [1080/50782], Loss: 5.7161\n",
            "Epoch [1/10], Step [1090/50782], Loss: 7.6141\n",
            "Epoch [1/10], Step [1100/50782], Loss: 7.9022\n",
            "Epoch [1/10], Step [1110/50782], Loss: 7.6766\n",
            "Epoch [1/10], Step [1120/50782], Loss: 6.7145\n",
            "Epoch [1/10], Step [1130/50782], Loss: 8.2802\n",
            "Epoch [1/10], Step [1140/50782], Loss: 8.1989\n",
            "Epoch [1/10], Step [1150/50782], Loss: 8.1355\n",
            "Epoch [1/10], Step [1160/50782], Loss: 7.4980\n",
            "Epoch [1/10], Step [1170/50782], Loss: 8.7102\n",
            "Epoch [1/10], Step [1180/50782], Loss: 6.4590\n",
            "Epoch [1/10], Step [1190/50782], Loss: 6.3642\n",
            "Epoch [1/10], Step [1200/50782], Loss: 6.2220\n",
            "Epoch [1/10], Step [1210/50782], Loss: 7.0576\n",
            "Epoch [1/10], Step [1220/50782], Loss: 8.9018\n",
            "Epoch [1/10], Step [1230/50782], Loss: 7.6678\n",
            "Epoch [1/10], Step [1240/50782], Loss: 7.4568\n",
            "Epoch [1/10], Step [1250/50782], Loss: 7.7658\n",
            "Epoch [1/10], Step [1260/50782], Loss: 7.5668\n",
            "Epoch [1/10], Step [1270/50782], Loss: 7.0653\n",
            "Epoch [1/10], Step [1280/50782], Loss: 7.0237\n",
            "Epoch [1/10], Step [1290/50782], Loss: 6.8636\n",
            "Epoch [1/10], Step [1300/50782], Loss: 6.5593\n",
            "Epoch [1/10], Step [1310/50782], Loss: 7.6529\n",
            "Epoch [1/10], Step [1320/50782], Loss: 7.3840\n",
            "Epoch [1/10], Step [1330/50782], Loss: 7.4153\n",
            "Epoch [1/10], Step [1340/50782], Loss: 8.0039\n",
            "Epoch [1/10], Step [1350/50782], Loss: 5.8423\n",
            "Epoch [1/10], Step [1360/50782], Loss: 8.3697\n",
            "Epoch [1/10], Step [1370/50782], Loss: 7.7699\n",
            "Epoch [1/10], Step [1380/50782], Loss: 7.3480\n",
            "Epoch [1/10], Step [1390/50782], Loss: 7.5434\n",
            "Epoch [1/10], Step [1400/50782], Loss: 9.4479\n",
            "Epoch [1/10], Step [1410/50782], Loss: 6.1884\n",
            "Epoch [1/10], Step [1420/50782], Loss: 8.2067\n",
            "Epoch [1/10], Step [1430/50782], Loss: 6.2296\n",
            "Epoch [1/10], Step [1440/50782], Loss: 7.2151\n",
            "Epoch [1/10], Step [1450/50782], Loss: 7.9881\n",
            "Epoch [1/10], Step [1460/50782], Loss: 7.7307\n",
            "Epoch [1/10], Step [1470/50782], Loss: 8.0149\n",
            "Epoch [1/10], Step [1480/50782], Loss: 6.8395\n",
            "Epoch [1/10], Step [1490/50782], Loss: 8.2727\n",
            "Epoch [1/10], Step [1500/50782], Loss: 7.6667\n",
            "Epoch [1/10], Step [1510/50782], Loss: 7.7555\n",
            "Epoch [1/10], Step [1520/50782], Loss: 8.6224\n",
            "Epoch [1/10], Step [1530/50782], Loss: 6.7072\n",
            "Epoch [1/10], Step [1540/50782], Loss: 6.4506\n",
            "Epoch [1/10], Step [1550/50782], Loss: 8.5696\n",
            "Epoch [1/10], Step [1560/50782], Loss: 6.9555\n",
            "Epoch [1/10], Step [1570/50782], Loss: 5.1568\n",
            "Epoch [1/10], Step [1580/50782], Loss: 8.1827\n",
            "Epoch [1/10], Step [1590/50782], Loss: 6.9404\n",
            "Epoch [1/10], Step [1600/50782], Loss: 6.5824\n",
            "Epoch [1/10], Step [1610/50782], Loss: 6.9533\n",
            "Epoch [1/10], Step [1620/50782], Loss: 7.0066\n",
            "Epoch [1/10], Step [1630/50782], Loss: 7.4637\n",
            "Epoch [1/10], Step [1640/50782], Loss: 8.3447\n",
            "Epoch [1/10], Step [1650/50782], Loss: 9.1963\n",
            "Epoch [1/10], Step [1660/50782], Loss: 8.1702\n",
            "Epoch [1/10], Step [1670/50782], Loss: 5.8962\n",
            "Epoch [1/10], Step [1680/50782], Loss: 7.2664\n",
            "Epoch [1/10], Step [1690/50782], Loss: 7.4883\n",
            "Epoch [1/10], Step [1700/50782], Loss: 7.4298\n",
            "Epoch [1/10], Step [1710/50782], Loss: 8.0616\n",
            "Epoch [1/10], Step [1720/50782], Loss: 7.9698\n",
            "Epoch [1/10], Step [1730/50782], Loss: 8.1080\n",
            "Epoch [1/10], Step [1740/50782], Loss: 7.3209\n",
            "Epoch [1/10], Step [1750/50782], Loss: 6.2511\n",
            "Epoch [1/10], Step [1760/50782], Loss: 7.9073\n",
            "Epoch [1/10], Step [1770/50782], Loss: 7.7627\n",
            "Epoch [1/10], Step [1780/50782], Loss: 8.0070\n",
            "Epoch [1/10], Step [1790/50782], Loss: 7.2069\n",
            "Epoch [1/10], Step [1800/50782], Loss: 7.3505\n",
            "Epoch [1/10], Step [1810/50782], Loss: 8.0688\n",
            "Epoch [1/10], Step [1820/50782], Loss: 7.9538\n",
            "Epoch [1/10], Step [1830/50782], Loss: 4.9015\n",
            "Epoch [1/10], Step [1840/50782], Loss: 6.4321\n",
            "Epoch [1/10], Step [1850/50782], Loss: 7.4187\n",
            "Epoch [1/10], Step [1860/50782], Loss: 6.2132\n",
            "Epoch [1/10], Step [1870/50782], Loss: 7.6780\n",
            "Epoch [1/10], Step [1880/50782], Loss: 8.2547\n",
            "Epoch [1/10], Step [1890/50782], Loss: 7.0047\n",
            "Epoch [1/10], Step [1900/50782], Loss: 6.7179\n",
            "Epoch [1/10], Step [1910/50782], Loss: 7.3320\n",
            "Epoch [1/10], Step [1920/50782], Loss: 7.9591\n",
            "Epoch [1/10], Step [1930/50782], Loss: 8.1158\n",
            "Epoch [1/10], Step [1940/50782], Loss: 7.4626\n",
            "Epoch [1/10], Step [1950/50782], Loss: 6.9935\n",
            "Epoch [1/10], Step [1960/50782], Loss: 7.0998\n",
            "Epoch [1/10], Step [1970/50782], Loss: 6.6327\n",
            "Epoch [1/10], Step [1980/50782], Loss: 7.3749\n",
            "Epoch [1/10], Step [1990/50782], Loss: 6.3488\n",
            "Epoch [1/10], Step [2000/50782], Loss: 6.7127\n",
            "Epoch [1/10], Step [2010/50782], Loss: 7.9068\n",
            "Epoch [1/10], Step [2020/50782], Loss: 9.5822\n",
            "Epoch [1/10], Step [2030/50782], Loss: 8.3712\n",
            "Epoch [1/10], Step [2040/50782], Loss: 8.3291\n",
            "Epoch [1/10], Step [2050/50782], Loss: 6.4016\n",
            "Epoch [1/10], Step [2060/50782], Loss: 6.8135\n",
            "Epoch [1/10], Step [2070/50782], Loss: 6.7964\n",
            "Epoch [1/10], Step [2080/50782], Loss: 5.6912\n",
            "Epoch [1/10], Step [2090/50782], Loss: 7.0399\n",
            "Epoch [1/10], Step [2100/50782], Loss: 7.3482\n",
            "Epoch [1/10], Step [2110/50782], Loss: 7.2146\n",
            "Epoch [1/10], Step [2120/50782], Loss: 7.0840\n",
            "Epoch [1/10], Step [2130/50782], Loss: 7.2160\n",
            "Epoch [1/10], Step [2140/50782], Loss: 6.4499\n",
            "Epoch [1/10], Step [2150/50782], Loss: 8.9972\n",
            "Epoch [1/10], Step [2160/50782], Loss: 8.1567\n",
            "Epoch [1/10], Step [2170/50782], Loss: 8.7461\n",
            "Epoch [1/10], Step [2180/50782], Loss: 7.3610\n",
            "Epoch [1/10], Step [2190/50782], Loss: 7.3773\n",
            "Epoch [1/10], Step [2200/50782], Loss: 5.0885\n",
            "Epoch [1/10], Step [2210/50782], Loss: 5.9714\n",
            "Epoch [1/10], Step [2220/50782], Loss: 8.3166\n",
            "Epoch [1/10], Step [2230/50782], Loss: 7.5629\n",
            "Epoch [1/10], Step [2240/50782], Loss: 8.1771\n",
            "Epoch [1/10], Step [2250/50782], Loss: 8.0154\n",
            "Epoch [1/10], Step [2260/50782], Loss: 7.4126\n",
            "Epoch [1/10], Step [2270/50782], Loss: 9.4931\n",
            "Epoch [1/10], Step [2280/50782], Loss: 7.9627\n",
            "Epoch [1/10], Step [2290/50782], Loss: 6.1873\n",
            "Epoch [1/10], Step [2300/50782], Loss: 7.7384\n",
            "Epoch [1/10], Step [2310/50782], Loss: 6.5892\n",
            "Epoch [1/10], Step [2320/50782], Loss: 7.3671\n",
            "Epoch [1/10], Step [2330/50782], Loss: 6.2227\n",
            "Epoch [1/10], Step [2340/50782], Loss: 7.6444\n",
            "Epoch [1/10], Step [2350/50782], Loss: 6.1656\n",
            "Epoch [1/10], Step [2360/50782], Loss: 7.0888\n",
            "Epoch [1/10], Step [2370/50782], Loss: 8.6654\n",
            "Epoch [1/10], Step [2380/50782], Loss: 6.7504\n",
            "Epoch [1/10], Step [2390/50782], Loss: 7.9448\n",
            "Epoch [1/10], Step [2400/50782], Loss: 6.6570\n",
            "Epoch [1/10], Step [2410/50782], Loss: 7.2673\n",
            "Epoch [1/10], Step [2420/50782], Loss: 7.5591\n",
            "Epoch [1/10], Step [2430/50782], Loss: 7.6251\n",
            "Epoch [1/10], Step [2440/50782], Loss: 8.7847\n",
            "Epoch [1/10], Step [2450/50782], Loss: 7.5380\n",
            "Epoch [1/10], Step [2460/50782], Loss: 6.0114\n",
            "Epoch [1/10], Step [2470/50782], Loss: 7.1776\n",
            "Epoch [1/10], Step [2480/50782], Loss: 7.7990\n",
            "Epoch [1/10], Step [2490/50782], Loss: 8.1547\n",
            "Epoch [1/10], Step [2500/50782], Loss: 7.7021\n",
            "Epoch [1/10], Step [2510/50782], Loss: 8.0672\n",
            "Epoch [1/10], Step [2520/50782], Loss: 6.4957\n",
            "Epoch [1/10], Step [2530/50782], Loss: 7.0576\n",
            "Epoch [1/10], Step [2540/50782], Loss: 7.5724\n",
            "Epoch [1/10], Step [2550/50782], Loss: 6.8650\n",
            "Epoch [1/10], Step [2560/50782], Loss: 6.4160\n",
            "Epoch [1/10], Step [2570/50782], Loss: 7.9359\n",
            "Epoch [1/10], Step [2580/50782], Loss: 9.0497\n",
            "Epoch [1/10], Step [2590/50782], Loss: 8.0857\n",
            "Epoch [1/10], Step [2600/50782], Loss: 6.7741\n",
            "Epoch [1/10], Step [2610/50782], Loss: 4.9159\n",
            "Epoch [1/10], Step [2620/50782], Loss: 5.9449\n",
            "Epoch [1/10], Step [2630/50782], Loss: 6.5482\n",
            "Epoch [1/10], Step [2640/50782], Loss: 7.9751\n",
            "Epoch [1/10], Step [2650/50782], Loss: 6.2431\n",
            "Epoch [1/10], Step [2660/50782], Loss: 8.0645\n",
            "Epoch [1/10], Step [2670/50782], Loss: 8.2711\n",
            "Epoch [1/10], Step [2680/50782], Loss: 8.7135\n",
            "Epoch [1/10], Step [2690/50782], Loss: 9.2883\n",
            "Epoch [1/10], Step [2700/50782], Loss: 7.2686\n",
            "Epoch [1/10], Step [2710/50782], Loss: 6.4346\n",
            "Epoch [1/10], Step [2720/50782], Loss: 6.3834\n",
            "Epoch [1/10], Step [2730/50782], Loss: 7.3185\n",
            "Epoch [1/10], Step [2740/50782], Loss: 9.1357\n",
            "Epoch [1/10], Step [2750/50782], Loss: 7.6084\n",
            "Epoch [1/10], Step [2760/50782], Loss: 7.4196\n",
            "Epoch [1/10], Step [2770/50782], Loss: 6.4996\n",
            "Epoch [1/10], Step [2780/50782], Loss: 7.1234\n",
            "Epoch [1/10], Step [2790/50782], Loss: 8.0241\n",
            "Epoch [1/10], Step [2800/50782], Loss: 6.4250\n",
            "Epoch [1/10], Step [2810/50782], Loss: 7.4324\n",
            "Epoch [1/10], Step [2820/50782], Loss: 8.2451\n",
            "Epoch [1/10], Step [2830/50782], Loss: 6.3456\n",
            "Epoch [1/10], Step [2840/50782], Loss: 8.5390\n",
            "Epoch [1/10], Step [2850/50782], Loss: 6.9488\n",
            "Epoch [1/10], Step [2860/50782], Loss: 7.3660\n",
            "Epoch [1/10], Step [2870/50782], Loss: 6.5440\n",
            "Epoch [1/10], Step [2880/50782], Loss: 6.5313\n",
            "Epoch [1/10], Step [2890/50782], Loss: 6.6670\n",
            "Epoch [1/10], Step [2900/50782], Loss: 6.2521\n",
            "Epoch [1/10], Step [2910/50782], Loss: 9.4596\n",
            "Epoch [1/10], Step [2920/50782], Loss: 6.6660\n",
            "Epoch [1/10], Step [2930/50782], Loss: 7.4445\n",
            "Epoch [1/10], Step [2940/50782], Loss: 6.5294\n",
            "Epoch [1/10], Step [2950/50782], Loss: 7.7096\n",
            "Epoch [1/10], Step [2960/50782], Loss: 6.2881\n",
            "Epoch [1/10], Step [2970/50782], Loss: 6.9049\n",
            "Epoch [1/10], Step [2980/50782], Loss: 6.9346\n",
            "Epoch [1/10], Step [2990/50782], Loss: 8.1586\n",
            "Epoch [1/10], Step [3000/50782], Loss: 8.7240\n",
            "Epoch [1/10], Step [3010/50782], Loss: 7.8038\n",
            "Epoch [1/10], Step [3020/50782], Loss: 8.4789\n",
            "Epoch [1/10], Step [3030/50782], Loss: 6.0068\n",
            "Epoch [1/10], Step [3040/50782], Loss: 6.7532\n",
            "Epoch [1/10], Step [3050/50782], Loss: 6.5771\n",
            "Epoch [1/10], Step [3060/50782], Loss: 7.5135\n",
            "Epoch [1/10], Step [3070/50782], Loss: 6.0725\n",
            "Epoch [1/10], Step [3080/50782], Loss: 6.8577\n",
            "Epoch [1/10], Step [3090/50782], Loss: 6.8978\n",
            "Epoch [1/10], Step [3100/50782], Loss: 5.6985\n",
            "Epoch [1/10], Step [3110/50782], Loss: 7.6852\n",
            "Epoch [1/10], Step [3120/50782], Loss: 8.8465\n",
            "Epoch [1/10], Step [3130/50782], Loss: 7.4437\n",
            "Epoch [1/10], Step [3140/50782], Loss: 7.3803\n",
            "Epoch [1/10], Step [3150/50782], Loss: 7.5014\n",
            "Epoch [1/10], Step [3160/50782], Loss: 8.1057\n",
            "Epoch [1/10], Step [3170/50782], Loss: 9.7645\n",
            "Epoch [1/10], Step [3180/50782], Loss: 8.2679\n",
            "Epoch [1/10], Step [3190/50782], Loss: 6.9075\n",
            "Epoch [1/10], Step [3200/50782], Loss: 8.1446\n",
            "Epoch [1/10], Step [3210/50782], Loss: 5.7441\n",
            "Epoch [1/10], Step [3220/50782], Loss: 8.0891\n",
            "Epoch [1/10], Step [3230/50782], Loss: 7.0468\n",
            "Epoch [1/10], Step [3240/50782], Loss: 7.9845\n",
            "Epoch [1/10], Step [3250/50782], Loss: 6.4099\n",
            "Epoch [1/10], Step [3260/50782], Loss: 6.9261\n",
            "Epoch [1/10], Step [3270/50782], Loss: 7.3300\n",
            "Epoch [1/10], Step [3280/50782], Loss: 7.4291\n",
            "Epoch [1/10], Step [3290/50782], Loss: 6.5419\n",
            "Epoch [1/10], Step [3300/50782], Loss: 8.4761\n",
            "Epoch [1/10], Step [3310/50782], Loss: 7.7271\n",
            "Epoch [1/10], Step [3320/50782], Loss: 6.9672\n",
            "Epoch [1/10], Step [3330/50782], Loss: 6.9533\n",
            "Epoch [1/10], Step [3340/50782], Loss: 8.2693\n",
            "Epoch [1/10], Step [3350/50782], Loss: 8.2824\n",
            "Epoch [1/10], Step [3360/50782], Loss: 7.8017\n",
            "Epoch [1/10], Step [3370/50782], Loss: 8.5639\n",
            "Epoch [1/10], Step [3380/50782], Loss: 7.3973\n",
            "Epoch [1/10], Step [3390/50782], Loss: 7.7701\n",
            "Epoch [1/10], Step [3400/50782], Loss: 6.6236\n",
            "Epoch [1/10], Step [3410/50782], Loss: 9.0703\n",
            "Epoch [1/10], Step [3420/50782], Loss: 5.8722\n",
            "Epoch [1/10], Step [3430/50782], Loss: 6.5793\n",
            "Epoch [1/10], Step [3440/50782], Loss: 6.7953\n",
            "Epoch [1/10], Step [3450/50782], Loss: 6.3125\n",
            "Epoch [1/10], Step [3460/50782], Loss: 8.2791\n",
            "Epoch [1/10], Step [3470/50782], Loss: 6.2897\n",
            "Epoch [1/10], Step [3480/50782], Loss: 7.3932\n",
            "Epoch [1/10], Step [3490/50782], Loss: 6.7695\n",
            "Epoch [1/10], Step [3500/50782], Loss: 7.4617\n",
            "Epoch [1/10], Step [3510/50782], Loss: 5.8339\n",
            "Epoch [1/10], Step [3520/50782], Loss: 7.7238\n",
            "Epoch [1/10], Step [3530/50782], Loss: 8.0494\n",
            "Epoch [1/10], Step [3540/50782], Loss: 7.7221\n",
            "Epoch [1/10], Step [3550/50782], Loss: 6.5534\n",
            "Epoch [1/10], Step [3560/50782], Loss: 6.5849\n",
            "Epoch [1/10], Step [3570/50782], Loss: 7.1816\n",
            "Epoch [1/10], Step [3580/50782], Loss: 7.2026\n",
            "Epoch [1/10], Step [3590/50782], Loss: 7.3819\n",
            "Epoch [1/10], Step [3600/50782], Loss: 8.5761\n",
            "Epoch [1/10], Step [3610/50782], Loss: 7.7195\n",
            "Epoch [1/10], Step [3620/50782], Loss: 8.9609\n",
            "Epoch [1/10], Step [3630/50782], Loss: 6.7860\n",
            "Epoch [1/10], Step [3640/50782], Loss: 8.1412\n",
            "Epoch [1/10], Step [3650/50782], Loss: 7.5695\n",
            "Epoch [1/10], Step [3660/50782], Loss: 6.3826\n",
            "Epoch [1/10], Step [3670/50782], Loss: 7.2292\n",
            "Epoch [1/10], Step [3680/50782], Loss: 6.8075\n",
            "Epoch [1/10], Step [3690/50782], Loss: 6.4097\n",
            "Epoch [1/10], Step [3700/50782], Loss: 7.4425\n",
            "Epoch [1/10], Step [3710/50782], Loss: 9.2444\n",
            "Epoch [1/10], Step [3720/50782], Loss: 6.9619\n",
            "Epoch [1/10], Step [3730/50782], Loss: 6.4629\n",
            "Epoch [1/10], Step [3740/50782], Loss: 7.4158\n",
            "Epoch [1/10], Step [3750/50782], Loss: 8.1645\n",
            "Epoch [1/10], Step [3760/50782], Loss: 7.1679\n",
            "Epoch [1/10], Step [3770/50782], Loss: 7.8470\n",
            "Epoch [1/10], Step [3780/50782], Loss: 5.9056\n",
            "Epoch [1/10], Step [3790/50782], Loss: 7.7973\n",
            "Epoch [1/10], Step [3800/50782], Loss: 6.5305\n",
            "Epoch [1/10], Step [3810/50782], Loss: 6.0665\n",
            "Epoch [1/10], Step [3820/50782], Loss: 7.0544\n",
            "Epoch [1/10], Step [3830/50782], Loss: 7.9056\n",
            "Epoch [1/10], Step [3840/50782], Loss: 7.6212\n",
            "Epoch [1/10], Step [3850/50782], Loss: 8.5719\n",
            "Epoch [1/10], Step [3860/50782], Loss: 7.3937\n",
            "Epoch [1/10], Step [3870/50782], Loss: 7.6118\n",
            "Epoch [1/10], Step [3880/50782], Loss: 8.5587\n",
            "Epoch [1/10], Step [3890/50782], Loss: 6.5360\n",
            "Epoch [1/10], Step [3900/50782], Loss: 8.1589\n",
            "Epoch [1/10], Step [3910/50782], Loss: 7.2399\n",
            "Epoch [1/10], Step [3920/50782], Loss: 7.0170\n",
            "Epoch [1/10], Step [3930/50782], Loss: 7.2201\n",
            "Epoch [1/10], Step [3940/50782], Loss: 7.4002\n",
            "Epoch [1/10], Step [3950/50782], Loss: 7.0663\n",
            "Epoch [1/10], Step [3960/50782], Loss: 7.5705\n",
            "Epoch [1/10], Step [3970/50782], Loss: 8.2921\n",
            "Epoch [1/10], Step [3980/50782], Loss: 6.7512\n",
            "Epoch [1/10], Step [3990/50782], Loss: 6.2592\n",
            "Epoch [1/10], Step [4000/50782], Loss: 5.8150\n",
            "Epoch [1/10], Step [4010/50782], Loss: 7.7309\n",
            "Epoch [1/10], Step [4020/50782], Loss: 8.2489\n",
            "Epoch [1/10], Step [4030/50782], Loss: 7.2362\n",
            "Epoch [1/10], Step [4040/50782], Loss: 7.5043\n",
            "Epoch [1/10], Step [4050/50782], Loss: 6.0862\n",
            "Epoch [1/10], Step [4060/50782], Loss: 7.9569\n",
            "Epoch [1/10], Step [4070/50782], Loss: 7.1761\n",
            "Epoch [1/10], Step [4080/50782], Loss: 7.1516\n",
            "Epoch [1/10], Step [4090/50782], Loss: 7.3506\n",
            "Epoch [1/10], Step [4100/50782], Loss: 7.7564\n",
            "Epoch [1/10], Step [4110/50782], Loss: 7.7325\n",
            "Epoch [1/10], Step [4120/50782], Loss: 8.0431\n",
            "Epoch [1/10], Step [4130/50782], Loss: 6.0872\n",
            "Epoch [1/10], Step [4140/50782], Loss: 7.7163\n",
            "Epoch [1/10], Step [4150/50782], Loss: 6.1754\n",
            "Epoch [1/10], Step [4160/50782], Loss: 8.6555\n",
            "Epoch [1/10], Step [4170/50782], Loss: 7.3024\n",
            "Epoch [1/10], Step [4180/50782], Loss: 6.9038\n",
            "Epoch [1/10], Step [4190/50782], Loss: 7.2854\n",
            "Epoch [1/10], Step [4200/50782], Loss: 6.4626\n",
            "Epoch [1/10], Step [4210/50782], Loss: 7.6828\n",
            "Epoch [1/10], Step [4220/50782], Loss: 6.3567\n",
            "Epoch [1/10], Step [4230/50782], Loss: 7.7880\n",
            "Epoch [1/10], Step [4240/50782], Loss: 6.8145\n",
            "Epoch [1/10], Step [4250/50782], Loss: 7.3135\n",
            "Epoch [1/10], Step [4260/50782], Loss: 6.5843\n",
            "Epoch [1/10], Step [4270/50782], Loss: 8.5160\n",
            "Epoch [1/10], Step [4280/50782], Loss: 6.4163\n",
            "Epoch [1/10], Step [4290/50782], Loss: 7.8598\n",
            "Epoch [1/10], Step [4300/50782], Loss: 9.6084\n",
            "Epoch [1/10], Step [4310/50782], Loss: 5.5616\n",
            "Epoch [1/10], Step [4320/50782], Loss: 7.4327\n",
            "Epoch [1/10], Step [4330/50782], Loss: 7.2862\n",
            "Epoch [1/10], Step [4340/50782], Loss: 6.4310\n",
            "Epoch [1/10], Step [4350/50782], Loss: 5.8893\n",
            "Epoch [1/10], Step [4360/50782], Loss: 7.0356\n",
            "Epoch [1/10], Step [4370/50782], Loss: 7.3544\n",
            "Epoch [1/10], Step [4380/50782], Loss: 9.0264\n",
            "Epoch [1/10], Step [4390/50782], Loss: 9.4984\n",
            "Epoch [1/10], Step [4400/50782], Loss: 6.4469\n",
            "Epoch [1/10], Step [4410/50782], Loss: 6.5733\n",
            "Epoch [1/10], Step [4420/50782], Loss: 7.0344\n",
            "Epoch [1/10], Step [4430/50782], Loss: 6.8339\n",
            "Epoch [1/10], Step [4440/50782], Loss: 7.7187\n",
            "Epoch [1/10], Step [4450/50782], Loss: 7.4266\n",
            "Epoch [1/10], Step [4460/50782], Loss: 6.4202\n",
            "Epoch [1/10], Step [4470/50782], Loss: 8.5044\n",
            "Epoch [1/10], Step [4480/50782], Loss: 8.2042\n",
            "Epoch [1/10], Step [4490/50782], Loss: 7.1666\n",
            "Epoch [1/10], Step [4500/50782], Loss: 6.1426\n",
            "Epoch [1/10], Step [4510/50782], Loss: 6.2783\n",
            "Epoch [1/10], Step [4520/50782], Loss: 5.8596\n",
            "Epoch [1/10], Step [4530/50782], Loss: 8.3835\n",
            "Epoch [1/10], Step [4540/50782], Loss: 8.0418\n",
            "Epoch [1/10], Step [4550/50782], Loss: 7.5322\n",
            "Epoch [1/10], Step [4560/50782], Loss: 7.8229\n",
            "Epoch [1/10], Step [4570/50782], Loss: 7.9408\n",
            "Epoch [1/10], Step [4580/50782], Loss: 6.6571\n",
            "Epoch [1/10], Step [4590/50782], Loss: 8.1520\n",
            "Epoch [1/10], Step [4600/50782], Loss: 6.9080\n",
            "Epoch [1/10], Step [4610/50782], Loss: 6.8749\n",
            "Epoch [1/10], Step [4620/50782], Loss: 7.6516\n",
            "Epoch [1/10], Step [4630/50782], Loss: 7.6050\n",
            "Epoch [1/10], Step [4640/50782], Loss: 7.5616\n",
            "Epoch [1/10], Step [4650/50782], Loss: 6.6133\n",
            "Epoch [1/10], Step [4660/50782], Loss: 7.1699\n",
            "Epoch [1/10], Step [4670/50782], Loss: 5.1738\n",
            "Epoch [1/10], Step [4680/50782], Loss: 7.8907\n",
            "Epoch [1/10], Step [4690/50782], Loss: 7.1931\n",
            "Epoch [1/10], Step [4700/50782], Loss: 5.9499\n",
            "Epoch [1/10], Step [4710/50782], Loss: 8.3265\n",
            "Epoch [1/10], Step [4720/50782], Loss: 6.7360\n",
            "Epoch [1/10], Step [4730/50782], Loss: 7.5438\n",
            "Epoch [1/10], Step [4740/50782], Loss: 9.1210\n",
            "Epoch [1/10], Step [4750/50782], Loss: 7.9028\n",
            "Epoch [1/10], Step [4760/50782], Loss: 7.5712\n",
            "Epoch [1/10], Step [4770/50782], Loss: 5.8118\n",
            "Epoch [1/10], Step [4780/50782], Loss: 8.0942\n",
            "Epoch [1/10], Step [4790/50782], Loss: 6.9148\n",
            "Epoch [1/10], Step [4800/50782], Loss: 6.9628\n",
            "Epoch [1/10], Step [4810/50782], Loss: 8.2045\n",
            "Epoch [1/10], Step [4820/50782], Loss: 6.9660\n",
            "Epoch [1/10], Step [4830/50782], Loss: 6.2086\n",
            "Epoch [1/10], Step [4840/50782], Loss: 7.4271\n",
            "Epoch [1/10], Step [4850/50782], Loss: 7.7952\n",
            "Epoch [1/10], Step [4860/50782], Loss: 9.3588\n",
            "Epoch [1/10], Step [4870/50782], Loss: 5.9223\n",
            "Epoch [1/10], Step [4880/50782], Loss: 5.7974\n",
            "Epoch [1/10], Step [4890/50782], Loss: 6.7004\n",
            "Epoch [1/10], Step [4900/50782], Loss: 6.6648\n",
            "Epoch [1/10], Step [4910/50782], Loss: 7.7501\n",
            "Epoch [1/10], Step [4920/50782], Loss: 7.4638\n",
            "Epoch [1/10], Step [4930/50782], Loss: 9.2334\n",
            "Epoch [1/10], Step [4940/50782], Loss: 6.3623\n",
            "Epoch [1/10], Step [4950/50782], Loss: 7.0849\n",
            "Epoch [1/10], Step [4960/50782], Loss: 6.4012\n",
            "Epoch [1/10], Step [4970/50782], Loss: 7.0807\n",
            "Epoch [1/10], Step [4980/50782], Loss: 6.3246\n",
            "Epoch [1/10], Step [4990/50782], Loss: 8.0220\n",
            "Epoch [1/10], Step [5000/50782], Loss: 9.1292\n",
            "Epoch [1/10], Step [5010/50782], Loss: 8.2627\n",
            "Epoch [1/10], Step [5020/50782], Loss: 7.4925\n",
            "Epoch [1/10], Step [5030/50782], Loss: 6.5799\n",
            "Epoch [1/10], Step [5040/50782], Loss: 7.1599\n",
            "Epoch [1/10], Step [5050/50782], Loss: 7.4820\n",
            "Epoch [1/10], Step [5060/50782], Loss: 9.4057\n",
            "Epoch [1/10], Step [5070/50782], Loss: 7.5733\n",
            "Epoch [1/10], Step [5080/50782], Loss: 8.3389\n",
            "Epoch [1/10], Step [5090/50782], Loss: 6.8852\n",
            "Epoch [1/10], Step [5100/50782], Loss: 6.4049\n",
            "Epoch [1/10], Step [5110/50782], Loss: 7.2645\n",
            "Epoch [1/10], Step [5120/50782], Loss: 6.2265\n",
            "Epoch [1/10], Step [5130/50782], Loss: 7.2752\n",
            "Epoch [1/10], Step [5140/50782], Loss: 6.4981\n",
            "Epoch [1/10], Step [5150/50782], Loss: 6.2017\n",
            "Epoch [1/10], Step [5160/50782], Loss: 6.6220\n",
            "Epoch [1/10], Step [5170/50782], Loss: 7.5203\n",
            "Epoch [1/10], Step [5180/50782], Loss: 7.7280\n",
            "Epoch [1/10], Step [5190/50782], Loss: 5.7266\n",
            "Epoch [1/10], Step [5200/50782], Loss: 6.7121\n",
            "Epoch [1/10], Step [5210/50782], Loss: 7.9529\n",
            "Epoch [1/10], Step [5220/50782], Loss: 7.8137\n",
            "Epoch [1/10], Step [5230/50782], Loss: 7.4905\n",
            "Epoch [1/10], Step [5240/50782], Loss: 8.0866\n",
            "Epoch [1/10], Step [5250/50782], Loss: 5.3538\n",
            "Epoch [1/10], Step [5260/50782], Loss: 7.3114\n",
            "Epoch [1/10], Step [5270/50782], Loss: 7.9159\n",
            "Epoch [1/10], Step [5280/50782], Loss: 6.9706\n",
            "Epoch [1/10], Step [5290/50782], Loss: 8.3532\n",
            "Epoch [1/10], Step [5300/50782], Loss: 7.9913\n",
            "Epoch [1/10], Step [5310/50782], Loss: 6.5296\n",
            "Epoch [1/10], Step [5320/50782], Loss: 6.6223\n",
            "Epoch [1/10], Step [5330/50782], Loss: 6.4297\n",
            "Epoch [1/10], Step [5340/50782], Loss: 8.6019\n",
            "Epoch [1/10], Step [5350/50782], Loss: 7.0376\n",
            "Epoch [1/10], Step [5360/50782], Loss: 6.2736\n",
            "Epoch [1/10], Step [5370/50782], Loss: 7.2543\n",
            "Epoch [1/10], Step [5380/50782], Loss: 8.5109\n",
            "Epoch [1/10], Step [5390/50782], Loss: 6.8946\n",
            "Epoch [1/10], Step [5400/50782], Loss: 7.7164\n",
            "Epoch [1/10], Step [5410/50782], Loss: 7.9844\n",
            "Epoch [1/10], Step [5420/50782], Loss: 7.8599\n",
            "Epoch [1/10], Step [5430/50782], Loss: 5.7002\n",
            "Epoch [1/10], Step [5440/50782], Loss: 7.7963\n",
            "Epoch [1/10], Step [5450/50782], Loss: 6.6791\n",
            "Epoch [1/10], Step [5460/50782], Loss: 7.9836\n",
            "Epoch [1/10], Step [5470/50782], Loss: 6.9713\n",
            "Epoch [1/10], Step [5480/50782], Loss: 7.6199\n",
            "Epoch [1/10], Step [5490/50782], Loss: 7.2135\n",
            "Epoch [1/10], Step [5500/50782], Loss: 8.2912\n",
            "Epoch [1/10], Step [5510/50782], Loss: 7.3332\n",
            "Epoch [1/10], Step [5520/50782], Loss: 6.6630\n",
            "Epoch [1/10], Step [5530/50782], Loss: 6.5304\n",
            "Epoch [1/10], Step [5540/50782], Loss: 7.5262\n",
            "Epoch [1/10], Step [5550/50782], Loss: 6.6561\n",
            "Epoch [1/10], Step [5560/50782], Loss: 7.9419\n",
            "Epoch [1/10], Step [5570/50782], Loss: 7.6168\n",
            "Epoch [1/10], Step [5580/50782], Loss: 5.3449\n",
            "Epoch [1/10], Step [5590/50782], Loss: 8.2592\n",
            "Epoch [1/10], Step [5600/50782], Loss: 6.5286\n",
            "Epoch [1/10], Step [5610/50782], Loss: 5.9209\n",
            "Epoch [1/10], Step [5620/50782], Loss: 7.0787\n",
            "Epoch [1/10], Step [5630/50782], Loss: 6.9637\n",
            "Epoch [1/10], Step [5640/50782], Loss: 5.6991\n",
            "Epoch [1/10], Step [5650/50782], Loss: 6.2400\n",
            "Epoch [1/10], Step [5660/50782], Loss: 8.7490\n",
            "Epoch [1/10], Step [5670/50782], Loss: 7.2886\n",
            "Epoch [1/10], Step [5680/50782], Loss: 6.5035\n",
            "Epoch [1/10], Step [5690/50782], Loss: 6.9903\n",
            "Epoch [1/10], Step [5700/50782], Loss: 5.9646\n",
            "Epoch [1/10], Step [5710/50782], Loss: 5.8211\n",
            "Epoch [1/10], Step [5720/50782], Loss: 7.5549\n",
            "Epoch [1/10], Step [5730/50782], Loss: 6.3493\n",
            "Epoch [1/10], Step [5740/50782], Loss: 7.6162\n",
            "Epoch [1/10], Step [5750/50782], Loss: 7.3580\n",
            "Epoch [1/10], Step [5760/50782], Loss: 7.2363\n",
            "Epoch [1/10], Step [5770/50782], Loss: 7.7145\n",
            "Epoch [1/10], Step [5780/50782], Loss: 7.2222\n",
            "Epoch [1/10], Step [5790/50782], Loss: 7.4074\n",
            "Epoch [1/10], Step [5800/50782], Loss: 5.4030\n",
            "Epoch [1/10], Step [5810/50782], Loss: 5.2328\n",
            "Epoch [1/10], Step [5820/50782], Loss: 6.8144\n",
            "Epoch [1/10], Step [5830/50782], Loss: 5.2112\n",
            "Epoch [1/10], Step [5840/50782], Loss: 6.1429\n",
            "Epoch [1/10], Step [5850/50782], Loss: 8.7354\n",
            "Epoch [1/10], Step [5860/50782], Loss: 7.9510\n",
            "Epoch [1/10], Step [5870/50782], Loss: 7.2789\n",
            "Epoch [1/10], Step [5880/50782], Loss: 6.5980\n",
            "Epoch [1/10], Step [5890/50782], Loss: 7.0927\n",
            "Epoch [1/10], Step [5900/50782], Loss: 8.1445\n",
            "Epoch [1/10], Step [5910/50782], Loss: 7.3041\n",
            "Epoch [1/10], Step [5920/50782], Loss: 6.4902\n",
            "Epoch [1/10], Step [5930/50782], Loss: 6.9732\n",
            "Epoch [1/10], Step [5940/50782], Loss: 7.0081\n",
            "Epoch [1/10], Step [5950/50782], Loss: 8.8136\n",
            "Epoch [1/10], Step [5960/50782], Loss: 6.9886\n",
            "Epoch [1/10], Step [5970/50782], Loss: 7.6100\n",
            "Epoch [1/10], Step [5980/50782], Loss: 7.4087\n",
            "Epoch [1/10], Step [5990/50782], Loss: 6.7646\n",
            "Epoch [1/10], Step [6000/50782], Loss: 7.2426\n",
            "Epoch [1/10], Step [6010/50782], Loss: 7.1206\n",
            "Epoch [1/10], Step [6020/50782], Loss: 7.7475\n",
            "Epoch [1/10], Step [6030/50782], Loss: 7.4535\n",
            "Epoch [1/10], Step [6040/50782], Loss: 6.7401\n",
            "Epoch [1/10], Step [6050/50782], Loss: 8.3284\n",
            "Epoch [1/10], Step [6060/50782], Loss: 6.4464\n",
            "Epoch [1/10], Step [6070/50782], Loss: 7.5052\n",
            "Epoch [1/10], Step [6080/50782], Loss: 6.6090\n",
            "Epoch [1/10], Step [6090/50782], Loss: 7.7330\n",
            "Epoch [1/10], Step [6100/50782], Loss: 7.3805\n",
            "Epoch [1/10], Step [6110/50782], Loss: 9.2952\n",
            "Epoch [1/10], Step [6120/50782], Loss: 7.6010\n",
            "Epoch [1/10], Step [6130/50782], Loss: 7.1697\n",
            "Epoch [1/10], Step [6140/50782], Loss: 8.3711\n",
            "Epoch [1/10], Step [6150/50782], Loss: 5.1177\n",
            "Epoch [1/10], Step [6160/50782], Loss: 8.0098\n",
            "Epoch [1/10], Step [6170/50782], Loss: 8.0250\n",
            "Epoch [1/10], Step [6180/50782], Loss: 7.9078\n",
            "Epoch [1/10], Step [6190/50782], Loss: 6.4750\n",
            "Epoch [1/10], Step [6200/50782], Loss: 6.9380\n",
            "Epoch [1/10], Step [6210/50782], Loss: 6.7967\n",
            "Epoch [1/10], Step [6220/50782], Loss: 8.2584\n",
            "Epoch [1/10], Step [6230/50782], Loss: 7.0874\n",
            "Epoch [1/10], Step [6240/50782], Loss: 6.8304\n",
            "Epoch [1/10], Step [6250/50782], Loss: 5.5099\n",
            "Epoch [1/10], Step [6260/50782], Loss: 8.1923\n",
            "Epoch [1/10], Step [6270/50782], Loss: 6.8514\n",
            "Epoch [1/10], Step [6280/50782], Loss: 6.4019\n",
            "Epoch [1/10], Step [6290/50782], Loss: 7.3065\n",
            "Epoch [1/10], Step [6300/50782], Loss: 6.3532\n",
            "Epoch [1/10], Step [6310/50782], Loss: 6.6316\n",
            "Epoch [1/10], Step [6320/50782], Loss: 7.1951\n",
            "Epoch [1/10], Step [6330/50782], Loss: 7.5954\n",
            "Epoch [1/10], Step [6340/50782], Loss: 6.6374\n",
            "Epoch [1/10], Step [6350/50782], Loss: 8.6773\n",
            "Epoch [1/10], Step [6360/50782], Loss: 8.5830\n",
            "Epoch [1/10], Step [6370/50782], Loss: 7.3026\n",
            "Epoch [1/10], Step [6380/50782], Loss: 7.4352\n",
            "Epoch [1/10], Step [6390/50782], Loss: 8.3631\n",
            "Epoch [1/10], Step [6400/50782], Loss: 7.7716\n",
            "Epoch [1/10], Step [6410/50782], Loss: 6.5486\n",
            "Epoch [1/10], Step [6420/50782], Loss: 8.2088\n",
            "Epoch [1/10], Step [6430/50782], Loss: 5.1957\n",
            "Epoch [1/10], Step [6440/50782], Loss: 7.0588\n",
            "Epoch [1/10], Step [6450/50782], Loss: 7.5720\n",
            "Epoch [1/10], Step [6460/50782], Loss: 7.2219\n",
            "Epoch [1/10], Step [6470/50782], Loss: 6.4501\n",
            "Epoch [1/10], Step [6480/50782], Loss: 7.9364\n",
            "Epoch [1/10], Step [6490/50782], Loss: 7.5207\n",
            "Epoch [1/10], Step [6500/50782], Loss: 8.3483\n",
            "Epoch [1/10], Step [6510/50782], Loss: 8.9579\n",
            "Epoch [1/10], Step [6520/50782], Loss: 7.3747\n",
            "Epoch [1/10], Step [6530/50782], Loss: 7.8655\n",
            "Epoch [1/10], Step [6540/50782], Loss: 6.9705\n",
            "Epoch [1/10], Step [6550/50782], Loss: 5.8951\n",
            "Epoch [1/10], Step [6560/50782], Loss: 7.5112\n",
            "Epoch [1/10], Step [6570/50782], Loss: 7.3025\n",
            "Epoch [1/10], Step [6580/50782], Loss: 7.8128\n",
            "Epoch [1/10], Step [6590/50782], Loss: 8.1012\n",
            "Epoch [1/10], Step [6600/50782], Loss: 7.4461\n",
            "Epoch [1/10], Step [6610/50782], Loss: 6.9961\n",
            "Epoch [1/10], Step [6620/50782], Loss: 7.3952\n",
            "Epoch [1/10], Step [6630/50782], Loss: 7.2881\n",
            "Epoch [1/10], Step [6640/50782], Loss: 6.7794\n",
            "Epoch [1/10], Step [6650/50782], Loss: 6.4901\n",
            "Epoch [1/10], Step [6660/50782], Loss: 7.4446\n",
            "Epoch [1/10], Step [6670/50782], Loss: 6.9853\n",
            "Epoch [1/10], Step [6680/50782], Loss: 8.0925\n",
            "Epoch [1/10], Step [6690/50782], Loss: 7.9330\n",
            "Epoch [1/10], Step [6700/50782], Loss: 8.3616\n",
            "Epoch [1/10], Step [6710/50782], Loss: 7.1489\n",
            "Epoch [1/10], Step [6720/50782], Loss: 7.3559\n",
            "Epoch [1/10], Step [6730/50782], Loss: 6.9950\n",
            "Epoch [1/10], Step [6740/50782], Loss: 7.1089\n",
            "Epoch [1/10], Step [6750/50782], Loss: 5.7385\n",
            "Epoch [1/10], Step [6760/50782], Loss: 8.4114\n",
            "Epoch [1/10], Step [6770/50782], Loss: 5.8592\n",
            "Epoch [1/10], Step [6780/50782], Loss: 6.5534\n",
            "Epoch [1/10], Step [6790/50782], Loss: 6.5005\n",
            "Epoch [1/10], Step [6800/50782], Loss: 7.1575\n",
            "Epoch [1/10], Step [6810/50782], Loss: 6.7086\n",
            "Epoch [1/10], Step [6820/50782], Loss: 5.7132\n",
            "Epoch [1/10], Step [6830/50782], Loss: 8.3980\n",
            "Epoch [1/10], Step [6840/50782], Loss: 5.4653\n",
            "Epoch [1/10], Step [6850/50782], Loss: 5.5159\n",
            "Epoch [1/10], Step [6860/50782], Loss: 8.3053\n",
            "Epoch [1/10], Step [6870/50782], Loss: 7.5102\n",
            "Epoch [1/10], Step [6880/50782], Loss: 7.9805\n",
            "Epoch [1/10], Step [6890/50782], Loss: 8.0160\n",
            "Epoch [1/10], Step [6900/50782], Loss: 8.5330\n",
            "Epoch [1/10], Step [6910/50782], Loss: 7.8193\n",
            "Epoch [1/10], Step [6920/50782], Loss: 7.3816\n",
            "Epoch [1/10], Step [6930/50782], Loss: 7.9435\n",
            "Epoch [1/10], Step [6940/50782], Loss: 6.6923\n",
            "Epoch [1/10], Step [6950/50782], Loss: 6.5705\n",
            "Epoch [1/10], Step [6960/50782], Loss: 9.0729\n",
            "Epoch [1/10], Step [6970/50782], Loss: 8.0993\n",
            "Epoch [1/10], Step [6980/50782], Loss: 6.0919\n",
            "Epoch [1/10], Step [6990/50782], Loss: 7.8734\n",
            "Epoch [1/10], Step [7000/50782], Loss: 5.5910\n",
            "Epoch [1/10], Step [7010/50782], Loss: 8.6402\n",
            "Epoch [1/10], Step [7020/50782], Loss: 6.5517\n",
            "Epoch [1/10], Step [7030/50782], Loss: 5.4992\n",
            "Epoch [1/10], Step [7040/50782], Loss: 5.9283\n",
            "Epoch [1/10], Step [7050/50782], Loss: 7.4113\n",
            "Epoch [1/10], Step [7060/50782], Loss: 7.9784\n",
            "Epoch [1/10], Step [7070/50782], Loss: 6.6953\n",
            "Epoch [1/10], Step [7080/50782], Loss: 7.3070\n",
            "Epoch [1/10], Step [7090/50782], Loss: 5.9204\n",
            "Epoch [1/10], Step [7100/50782], Loss: 6.9969\n",
            "Epoch [1/10], Step [7110/50782], Loss: 6.3455\n",
            "Epoch [1/10], Step [7120/50782], Loss: 6.3402\n",
            "Epoch [1/10], Step [7130/50782], Loss: 7.7303\n",
            "Epoch [1/10], Step [7140/50782], Loss: 7.4647\n",
            "Epoch [1/10], Step [7150/50782], Loss: 8.6955\n",
            "Epoch [1/10], Step [7160/50782], Loss: 7.7109\n",
            "Epoch [1/10], Step [7170/50782], Loss: 5.0752\n",
            "Epoch [1/10], Step [7180/50782], Loss: 7.8918\n",
            "Epoch [1/10], Step [7190/50782], Loss: 7.1522\n",
            "Epoch [1/10], Step [7200/50782], Loss: 8.9842\n",
            "Epoch [1/10], Step [7210/50782], Loss: 6.6349\n",
            "Epoch [1/10], Step [7220/50782], Loss: 6.7615\n",
            "Epoch [1/10], Step [7230/50782], Loss: 7.4570\n",
            "Epoch [1/10], Step [7240/50782], Loss: 6.5358\n",
            "Epoch [1/10], Step [7250/50782], Loss: 7.6104\n",
            "Epoch [1/10], Step [7260/50782], Loss: 7.2079\n",
            "Epoch [1/10], Step [7270/50782], Loss: 7.8578\n",
            "Epoch [1/10], Step [7280/50782], Loss: 6.1475\n",
            "Epoch [1/10], Step [7290/50782], Loss: 8.1324\n",
            "Epoch [1/10], Step [7300/50782], Loss: 9.4074\n",
            "Epoch [1/10], Step [7310/50782], Loss: 7.9617\n",
            "Epoch [1/10], Step [7320/50782], Loss: 6.8484\n",
            "Epoch [1/10], Step [7330/50782], Loss: 7.9359\n",
            "Epoch [1/10], Step [7340/50782], Loss: 8.0963\n",
            "Epoch [1/10], Step [7350/50782], Loss: 6.7151\n",
            "Epoch [1/10], Step [7360/50782], Loss: 7.7579\n",
            "Epoch [1/10], Step [7370/50782], Loss: 7.8520\n",
            "Epoch [1/10], Step [7380/50782], Loss: 7.2177\n",
            "Epoch [1/10], Step [7390/50782], Loss: 7.6772\n",
            "Epoch [1/10], Step [7400/50782], Loss: 7.3537\n",
            "Epoch [1/10], Step [7410/50782], Loss: 8.4994\n",
            "Epoch [1/10], Step [7420/50782], Loss: 6.5866\n",
            "Epoch [1/10], Step [7430/50782], Loss: 8.6890\n",
            "Epoch [1/10], Step [7440/50782], Loss: 7.7156\n",
            "Epoch [1/10], Step [7450/50782], Loss: 7.7088\n",
            "Epoch [1/10], Step [7460/50782], Loss: 8.1649\n",
            "Epoch [1/10], Step [7470/50782], Loss: 6.7101\n",
            "Epoch [1/10], Step [7480/50782], Loss: 7.3027\n",
            "Epoch [1/10], Step [7490/50782], Loss: 8.2442\n",
            "Epoch [1/10], Step [7500/50782], Loss: 7.2993\n",
            "Epoch [1/10], Step [7510/50782], Loss: 8.2161\n",
            "Epoch [1/10], Step [7520/50782], Loss: 8.2283\n",
            "Epoch [1/10], Step [7530/50782], Loss: 7.8896\n",
            "Epoch [1/10], Step [7540/50782], Loss: 7.6589\n",
            "Epoch [1/10], Step [7550/50782], Loss: 8.7734\n",
            "Epoch [1/10], Step [7560/50782], Loss: 7.0010\n",
            "Epoch [1/10], Step [7570/50782], Loss: 6.6273\n",
            "Epoch [1/10], Step [7580/50782], Loss: 7.5318\n",
            "Epoch [1/10], Step [7590/50782], Loss: 6.7489\n",
            "Epoch [1/10], Step [7600/50782], Loss: 5.9816\n",
            "Epoch [1/10], Step [7610/50782], Loss: 7.6371\n",
            "Epoch [1/10], Step [7620/50782], Loss: 8.0687\n",
            "Epoch [1/10], Step [7630/50782], Loss: 6.0662\n",
            "Epoch [1/10], Step [7640/50782], Loss: 7.9971\n",
            "Epoch [1/10], Step [7650/50782], Loss: 5.8672\n",
            "Epoch [1/10], Step [7660/50782], Loss: 5.3540\n",
            "Epoch [1/10], Step [7670/50782], Loss: 7.8271\n",
            "Epoch [1/10], Step [7680/50782], Loss: 5.8883\n",
            "Epoch [1/10], Step [7690/50782], Loss: 7.8713\n",
            "Epoch [1/10], Step [7700/50782], Loss: 7.3018\n",
            "Epoch [1/10], Step [7710/50782], Loss: 7.1564\n",
            "Epoch [1/10], Step [7720/50782], Loss: 7.3367\n",
            "Epoch [1/10], Step [7730/50782], Loss: 7.0021\n",
            "Epoch [1/10], Step [7740/50782], Loss: 6.1421\n",
            "Epoch [1/10], Step [7750/50782], Loss: 8.3168\n",
            "Epoch [1/10], Step [7760/50782], Loss: 5.8014\n",
            "Epoch [1/10], Step [7770/50782], Loss: 8.4818\n",
            "Epoch [1/10], Step [7780/50782], Loss: 6.9301\n",
            "Epoch [1/10], Step [7790/50782], Loss: 7.0871\n",
            "Epoch [1/10], Step [7800/50782], Loss: 8.0595\n",
            "Epoch [1/10], Step [7810/50782], Loss: 9.0254\n",
            "Epoch [1/10], Step [7820/50782], Loss: 6.5215\n",
            "Epoch [1/10], Step [7830/50782], Loss: 7.5446\n",
            "Epoch [1/10], Step [7840/50782], Loss: 7.2317\n",
            "Epoch [1/10], Step [7850/50782], Loss: 6.9745\n",
            "Epoch [1/10], Step [7860/50782], Loss: 7.3726\n",
            "Epoch [1/10], Step [7870/50782], Loss: 9.3568\n",
            "Epoch [1/10], Step [7880/50782], Loss: 8.1932\n",
            "Epoch [1/10], Step [7890/50782], Loss: 6.5254\n",
            "Epoch [1/10], Step [7900/50782], Loss: 8.0416\n",
            "Epoch [1/10], Step [7910/50782], Loss: 7.7370\n",
            "Epoch [1/10], Step [7920/50782], Loss: 8.6540\n",
            "Epoch [1/10], Step [7930/50782], Loss: 6.8966\n",
            "Epoch [1/10], Step [7940/50782], Loss: 8.3005\n",
            "Epoch [1/10], Step [7950/50782], Loss: 8.6921\n",
            "Epoch [1/10], Step [7960/50782], Loss: 6.1778\n",
            "Epoch [1/10], Step [7970/50782], Loss: 7.2317\n",
            "Epoch [1/10], Step [7980/50782], Loss: 8.9109\n",
            "Epoch [1/10], Step [7990/50782], Loss: 6.7716\n",
            "Epoch [1/10], Step [8000/50782], Loss: 7.8591\n",
            "Epoch [1/10], Step [8010/50782], Loss: 6.9660\n",
            "Epoch [1/10], Step [8020/50782], Loss: 7.3932\n",
            "Epoch [1/10], Step [8030/50782], Loss: 9.2386\n",
            "Epoch [1/10], Step [8040/50782], Loss: 7.4299\n",
            "Epoch [1/10], Step [8050/50782], Loss: 7.8957\n",
            "Epoch [1/10], Step [8060/50782], Loss: 7.4376\n",
            "Epoch [1/10], Step [8070/50782], Loss: 8.5630\n",
            "Epoch [1/10], Step [8080/50782], Loss: 6.9855\n",
            "Epoch [1/10], Step [8090/50782], Loss: 6.5541\n",
            "Epoch [1/10], Step [8100/50782], Loss: 6.7998\n",
            "Epoch [1/10], Step [8110/50782], Loss: 7.8960\n",
            "Epoch [1/10], Step [8120/50782], Loss: 8.6940\n",
            "Epoch [1/10], Step [8130/50782], Loss: 9.2756\n",
            "Epoch [1/10], Step [8140/50782], Loss: 7.3043\n",
            "Epoch [1/10], Step [8150/50782], Loss: 8.5197\n",
            "Epoch [1/10], Step [8160/50782], Loss: 7.6630\n",
            "Epoch [1/10], Step [8170/50782], Loss: 8.6010\n",
            "Epoch [1/10], Step [8180/50782], Loss: 7.3674\n",
            "Epoch [1/10], Step [8190/50782], Loss: 7.5862\n",
            "Epoch [1/10], Step [8200/50782], Loss: 7.4128\n",
            "Epoch [1/10], Step [8210/50782], Loss: 7.0652\n",
            "Epoch [1/10], Step [8220/50782], Loss: 7.5580\n",
            "Epoch [1/10], Step [8230/50782], Loss: 7.3102\n",
            "Epoch [1/10], Step [8240/50782], Loss: 8.4268\n",
            "Epoch [1/10], Step [8250/50782], Loss: 7.4904\n",
            "Epoch [1/10], Step [8260/50782], Loss: 6.3183\n",
            "Epoch [1/10], Step [8270/50782], Loss: 7.1459\n",
            "Epoch [1/10], Step [8280/50782], Loss: 6.0740\n",
            "Epoch [1/10], Step [8290/50782], Loss: 8.2684\n",
            "Epoch [1/10], Step [8300/50782], Loss: 6.1317\n",
            "Epoch [1/10], Step [8310/50782], Loss: 7.7692\n",
            "Epoch [1/10], Step [8320/50782], Loss: 8.1813\n",
            "Epoch [1/10], Step [8330/50782], Loss: 6.1838\n",
            "Epoch [1/10], Step [8340/50782], Loss: 7.1181\n",
            "Epoch [1/10], Step [8350/50782], Loss: 7.2456\n",
            "Epoch [1/10], Step [8360/50782], Loss: 7.1432\n",
            "Epoch [1/10], Step [8370/50782], Loss: 6.7641\n",
            "Epoch [1/10], Step [8380/50782], Loss: 7.5628\n",
            "Epoch [1/10], Step [8390/50782], Loss: 9.0330\n",
            "Epoch [1/10], Step [8400/50782], Loss: 6.8314\n",
            "Epoch [1/10], Step [8410/50782], Loss: 8.6543\n",
            "Epoch [1/10], Step [8420/50782], Loss: 8.7758\n",
            "Epoch [1/10], Step [8430/50782], Loss: 6.6519\n",
            "Epoch [1/10], Step [8440/50782], Loss: 7.4436\n",
            "Epoch [1/10], Step [8450/50782], Loss: 6.8154\n",
            "Epoch [1/10], Step [8460/50782], Loss: 7.6691\n",
            "Epoch [1/10], Step [8470/50782], Loss: 7.2509\n",
            "Epoch [1/10], Step [8480/50782], Loss: 8.5354\n",
            "Epoch [1/10], Step [8490/50782], Loss: 6.7214\n",
            "Epoch [1/10], Step [8500/50782], Loss: 6.1645\n",
            "Epoch [1/10], Step [8510/50782], Loss: 8.1042\n",
            "Epoch [1/10], Step [8520/50782], Loss: 6.9330\n",
            "Epoch [1/10], Step [8530/50782], Loss: 7.5907\n",
            "Epoch [1/10], Step [8540/50782], Loss: 7.8433\n",
            "Epoch [1/10], Step [8550/50782], Loss: 7.0116\n",
            "Epoch [1/10], Step [8560/50782], Loss: 7.6323\n",
            "Epoch [1/10], Step [8570/50782], Loss: 7.7229\n",
            "Epoch [1/10], Step [8580/50782], Loss: 7.8983\n",
            "Epoch [1/10], Step [8590/50782], Loss: 6.9248\n",
            "Epoch [1/10], Step [8600/50782], Loss: 7.6255\n",
            "Epoch [1/10], Step [8610/50782], Loss: 8.0069\n",
            "Epoch [1/10], Step [8620/50782], Loss: 7.8752\n",
            "Epoch [1/10], Step [8630/50782], Loss: 6.3540\n",
            "Epoch [1/10], Step [8640/50782], Loss: 7.0129\n",
            "Epoch [1/10], Step [8650/50782], Loss: 7.8438\n",
            "Epoch [1/10], Step [8660/50782], Loss: 6.5387\n",
            "Epoch [1/10], Step [8670/50782], Loss: 7.3764\n",
            "Epoch [1/10], Step [8680/50782], Loss: 6.6297\n",
            "Epoch [1/10], Step [8690/50782], Loss: 7.8141\n",
            "Epoch [1/10], Step [8700/50782], Loss: 8.3165\n",
            "Epoch [1/10], Step [8710/50782], Loss: 8.8618\n",
            "Epoch [1/10], Step [8720/50782], Loss: 6.2323\n",
            "Epoch [1/10], Step [8730/50782], Loss: 7.9141\n",
            "Epoch [1/10], Step [8740/50782], Loss: 8.7016\n",
            "Epoch [1/10], Step [8750/50782], Loss: 6.2588\n",
            "Epoch [1/10], Step [8760/50782], Loss: 7.4262\n",
            "Epoch [1/10], Step [8770/50782], Loss: 7.0442\n",
            "Epoch [1/10], Step [8780/50782], Loss: 5.9492\n",
            "Epoch [1/10], Step [8790/50782], Loss: 7.5118\n",
            "Epoch [1/10], Step [8800/50782], Loss: 6.7876\n",
            "Epoch [1/10], Step [8810/50782], Loss: 6.6285\n",
            "Epoch [1/10], Step [8820/50782], Loss: 7.4605\n",
            "Epoch [1/10], Step [8830/50782], Loss: 6.2624\n",
            "Epoch [1/10], Step [8840/50782], Loss: 7.6391\n",
            "Epoch [1/10], Step [8850/50782], Loss: 8.0139\n",
            "Epoch [1/10], Step [8860/50782], Loss: 7.6353\n",
            "Epoch [1/10], Step [8870/50782], Loss: 6.2542\n",
            "Epoch [1/10], Step [8880/50782], Loss: 7.6384\n",
            "Epoch [1/10], Step [8890/50782], Loss: 6.5420\n",
            "Epoch [1/10], Step [8900/50782], Loss: 6.5898\n",
            "Epoch [1/10], Step [8910/50782], Loss: 6.6876\n",
            "Epoch [1/10], Step [8920/50782], Loss: 7.4725\n",
            "Epoch [1/10], Step [8930/50782], Loss: 6.1931\n",
            "Epoch [1/10], Step [8940/50782], Loss: 7.1787\n",
            "Epoch [1/10], Step [8950/50782], Loss: 8.1610\n",
            "Epoch [1/10], Step [8960/50782], Loss: 7.0150\n",
            "Epoch [1/10], Step [8970/50782], Loss: 7.1724\n",
            "Epoch [1/10], Step [8980/50782], Loss: 7.3717\n",
            "Epoch [1/10], Step [8990/50782], Loss: 7.3832\n",
            "Epoch [1/10], Step [9000/50782], Loss: 7.8819\n",
            "Epoch [1/10], Step [9010/50782], Loss: 6.4473\n",
            "Epoch [1/10], Step [9020/50782], Loss: 7.1346\n",
            "Epoch [1/10], Step [9030/50782], Loss: 6.7054\n",
            "Epoch [1/10], Step [9040/50782], Loss: 8.3867\n",
            "Epoch [1/10], Step [9050/50782], Loss: 7.8847\n",
            "Epoch [1/10], Step [9060/50782], Loss: 6.5382\n",
            "Epoch [1/10], Step [9070/50782], Loss: 7.1594\n",
            "Epoch [1/10], Step [9080/50782], Loss: 7.8618\n",
            "Epoch [1/10], Step [9090/50782], Loss: 5.3677\n",
            "Epoch [1/10], Step [9100/50782], Loss: 7.7243\n",
            "Epoch [1/10], Step [9110/50782], Loss: 7.1084\n",
            "Epoch [1/10], Step [9120/50782], Loss: 7.6704\n",
            "Epoch [1/10], Step [9130/50782], Loss: 7.4831\n",
            "Epoch [1/10], Step [9140/50782], Loss: 8.2700\n",
            "Epoch [1/10], Step [9150/50782], Loss: 6.4634\n",
            "Epoch [1/10], Step [9160/50782], Loss: 5.7051\n",
            "Epoch [1/10], Step [9170/50782], Loss: 7.4723\n",
            "Epoch [1/10], Step [9180/50782], Loss: 6.0096\n",
            "Epoch [1/10], Step [9190/50782], Loss: 6.2600\n",
            "Epoch [1/10], Step [9200/50782], Loss: 6.0946\n",
            "Epoch [1/10], Step [9210/50782], Loss: 6.7415\n",
            "Epoch [1/10], Step [9220/50782], Loss: 6.6659\n",
            "Epoch [1/10], Step [9230/50782], Loss: 7.4808\n",
            "Epoch [1/10], Step [9240/50782], Loss: 7.5605\n",
            "Epoch [1/10], Step [9250/50782], Loss: 8.4046\n",
            "Epoch [1/10], Step [9260/50782], Loss: 6.1799\n",
            "Epoch [1/10], Step [9270/50782], Loss: 7.8231\n",
            "Epoch [1/10], Step [9280/50782], Loss: 6.1177\n",
            "Epoch [1/10], Step [9290/50782], Loss: 8.1005\n",
            "Epoch [1/10], Step [9300/50782], Loss: 8.4185\n",
            "Epoch [1/10], Step [9310/50782], Loss: 7.4230\n",
            "Epoch [1/10], Step [9320/50782], Loss: 6.4891\n",
            "Epoch [1/10], Step [9330/50782], Loss: 8.4553\n",
            "Epoch [1/10], Step [9340/50782], Loss: 7.2680\n",
            "Epoch [1/10], Step [9350/50782], Loss: 6.1079\n",
            "Epoch [1/10], Step [9360/50782], Loss: 6.5748\n",
            "Epoch [1/10], Step [9370/50782], Loss: 8.5221\n",
            "Epoch [1/10], Step [9380/50782], Loss: 7.6031\n",
            "Epoch [1/10], Step [9390/50782], Loss: 8.9178\n",
            "Epoch [1/10], Step [9400/50782], Loss: 8.7609\n",
            "Epoch [1/10], Step [9410/50782], Loss: 6.2855\n",
            "Epoch [1/10], Step [9420/50782], Loss: 8.5281\n",
            "Epoch [1/10], Step [9430/50782], Loss: 5.6222\n",
            "Epoch [1/10], Step [9440/50782], Loss: 7.9611\n",
            "Epoch [1/10], Step [9450/50782], Loss: 7.3045\n",
            "Epoch [1/10], Step [9460/50782], Loss: 7.3812\n",
            "Epoch [1/10], Step [9470/50782], Loss: 8.2117\n",
            "Epoch [1/10], Step [9480/50782], Loss: 6.6659\n",
            "Epoch [1/10], Step [9490/50782], Loss: 7.7721\n",
            "Epoch [1/10], Step [9500/50782], Loss: 6.2627\n",
            "Epoch [1/10], Step [9510/50782], Loss: 7.6596\n",
            "Epoch [1/10], Step [9520/50782], Loss: 5.9052\n",
            "Epoch [1/10], Step [9530/50782], Loss: 7.8979\n",
            "Epoch [1/10], Step [9540/50782], Loss: 8.9946\n",
            "Epoch [1/10], Step [9550/50782], Loss: 8.2711\n",
            "Epoch [1/10], Step [9560/50782], Loss: 7.0186\n",
            "Epoch [1/10], Step [9570/50782], Loss: 5.4666\n",
            "Epoch [1/10], Step [9580/50782], Loss: 7.7687\n",
            "Epoch [1/10], Step [9590/50782], Loss: 6.3938\n",
            "Epoch [1/10], Step [9600/50782], Loss: 6.4463\n",
            "Epoch [1/10], Step [9610/50782], Loss: 8.5468\n",
            "Epoch [1/10], Step [9620/50782], Loss: 8.8055\n",
            "Epoch [1/10], Step [9630/50782], Loss: 8.2887\n",
            "Epoch [1/10], Step [9640/50782], Loss: 8.0141\n",
            "Epoch [1/10], Step [9650/50782], Loss: 8.9871\n",
            "Epoch [1/10], Step [9660/50782], Loss: 8.0557\n",
            "Epoch [1/10], Step [9670/50782], Loss: 7.3769\n",
            "Epoch [1/10], Step [9680/50782], Loss: 7.1514\n",
            "Epoch [1/10], Step [9690/50782], Loss: 8.9688\n",
            "Epoch [1/10], Step [9700/50782], Loss: 5.2231\n",
            "Epoch [1/10], Step [9710/50782], Loss: 7.2362\n",
            "Epoch [1/10], Step [9720/50782], Loss: 7.4373\n",
            "Epoch [1/10], Step [9730/50782], Loss: 7.0356\n",
            "Epoch [1/10], Step [9740/50782], Loss: 7.1296\n",
            "Epoch [1/10], Step [9750/50782], Loss: 7.2389\n",
            "Epoch [1/10], Step [9760/50782], Loss: 7.4647\n",
            "Epoch [1/10], Step [9770/50782], Loss: 6.2718\n",
            "Epoch [1/10], Step [9780/50782], Loss: 6.5395\n",
            "Epoch [1/10], Step [9790/50782], Loss: 5.7740\n",
            "Epoch [1/10], Step [9800/50782], Loss: 8.1511\n",
            "Epoch [1/10], Step [9810/50782], Loss: 7.1865\n",
            "Epoch [1/10], Step [9820/50782], Loss: 6.1371\n",
            "Epoch [1/10], Step [9830/50782], Loss: 7.2423\n",
            "Epoch [1/10], Step [9840/50782], Loss: 8.0636\n",
            "Epoch [1/10], Step [9850/50782], Loss: 8.4433\n",
            "Epoch [1/10], Step [9860/50782], Loss: 5.2806\n",
            "Epoch [1/10], Step [9870/50782], Loss: 6.7500\n",
            "Epoch [1/10], Step [9880/50782], Loss: 7.2905\n",
            "Epoch [1/10], Step [9890/50782], Loss: 6.4957\n",
            "Epoch [1/10], Step [9900/50782], Loss: 7.3863\n",
            "Epoch [1/10], Step [9910/50782], Loss: 7.0551\n",
            "Epoch [1/10], Step [9920/50782], Loss: 6.0155\n",
            "Epoch [1/10], Step [9930/50782], Loss: 8.1648\n",
            "Epoch [1/10], Step [9940/50782], Loss: 6.6642\n",
            "Epoch [1/10], Step [9950/50782], Loss: 8.0361\n",
            "Epoch [1/10], Step [9960/50782], Loss: 8.3847\n",
            "Epoch [1/10], Step [9970/50782], Loss: 7.6590\n",
            "Epoch [1/10], Step [9980/50782], Loss: 7.2060\n",
            "Epoch [1/10], Step [9990/50782], Loss: 7.3390\n",
            "Epoch [1/10], Step [10000/50782], Loss: 8.5123\n",
            "Epoch [1/10], Step [10010/50782], Loss: 7.6838\n",
            "Epoch [1/10], Step [10020/50782], Loss: 7.3809\n",
            "Epoch [1/10], Step [10030/50782], Loss: 6.3198\n",
            "Epoch [1/10], Step [10040/50782], Loss: 6.3489\n",
            "Epoch [1/10], Step [10050/50782], Loss: 8.0060\n",
            "Epoch [1/10], Step [10060/50782], Loss: 8.2755\n",
            "Epoch [1/10], Step [10070/50782], Loss: 7.0474\n",
            "Epoch [1/10], Step [10080/50782], Loss: 6.9843\n",
            "Epoch [1/10], Step [10090/50782], Loss: 7.9916\n",
            "Epoch [1/10], Step [10100/50782], Loss: 7.9130\n",
            "Epoch [1/10], Step [10110/50782], Loss: 7.2557\n",
            "Epoch [1/10], Step [10120/50782], Loss: 8.8430\n",
            "Epoch [1/10], Step [10130/50782], Loss: 6.7024\n",
            "Epoch [1/10], Step [10140/50782], Loss: 8.5684\n",
            "Epoch [1/10], Step [10150/50782], Loss: 6.8663\n",
            "Epoch [1/10], Step [10160/50782], Loss: 7.4028\n",
            "Epoch [1/10], Step [10170/50782], Loss: 8.4212\n",
            "Epoch [1/10], Step [10180/50782], Loss: 7.2820\n",
            "Epoch [1/10], Step [10190/50782], Loss: 8.6853\n",
            "Epoch [1/10], Step [10200/50782], Loss: 7.8892\n",
            "Epoch [1/10], Step [10210/50782], Loss: 7.9953\n",
            "Epoch [1/10], Step [10220/50782], Loss: 6.9292\n",
            "Epoch [1/10], Step [10230/50782], Loss: 6.7931\n",
            "Epoch [1/10], Step [10240/50782], Loss: 7.9512\n",
            "Epoch [1/10], Step [10250/50782], Loss: 7.0699\n",
            "Epoch [1/10], Step [10260/50782], Loss: 6.3632\n",
            "Epoch [1/10], Step [10270/50782], Loss: 8.6285\n",
            "Epoch [1/10], Step [10280/50782], Loss: 7.3519\n",
            "Epoch [1/10], Step [10290/50782], Loss: 6.7679\n",
            "Epoch [1/10], Step [10300/50782], Loss: 6.5883\n",
            "Epoch [1/10], Step [10310/50782], Loss: 4.3296\n",
            "Epoch [1/10], Step [10320/50782], Loss: 6.8934\n",
            "Epoch [1/10], Step [10330/50782], Loss: 7.2540\n",
            "Epoch [1/10], Step [10340/50782], Loss: 6.8794\n",
            "Epoch [1/10], Step [10350/50782], Loss: 6.4087\n",
            "Epoch [1/10], Step [10360/50782], Loss: 7.7812\n",
            "Epoch [1/10], Step [10370/50782], Loss: 7.1588\n",
            "Epoch [1/10], Step [10380/50782], Loss: 7.3821\n",
            "Epoch [1/10], Step [10390/50782], Loss: 7.7853\n",
            "Epoch [1/10], Step [10400/50782], Loss: 6.6082\n",
            "Epoch [1/10], Step [10410/50782], Loss: 7.9586\n",
            "Epoch [1/10], Step [10420/50782], Loss: 7.5886\n",
            "Epoch [1/10], Step [10430/50782], Loss: 6.6241\n",
            "Epoch [1/10], Step [10440/50782], Loss: 7.4791\n",
            "Epoch [1/10], Step [10450/50782], Loss: 8.6024\n",
            "Epoch [1/10], Step [10460/50782], Loss: 8.1310\n",
            "Epoch [1/10], Step [10470/50782], Loss: 8.3308\n",
            "Epoch [1/10], Step [10480/50782], Loss: 6.6807\n",
            "Epoch [1/10], Step [10490/50782], Loss: 7.2706\n",
            "Epoch [1/10], Step [10500/50782], Loss: 8.9832\n",
            "Epoch [1/10], Step [10510/50782], Loss: 7.6891\n",
            "Epoch [1/10], Step [10520/50782], Loss: 7.4615\n",
            "Epoch [1/10], Step [10530/50782], Loss: 7.5678\n",
            "Epoch [1/10], Step [10540/50782], Loss: 5.1912\n",
            "Epoch [1/10], Step [10550/50782], Loss: 6.6045\n",
            "Epoch [1/10], Step [10560/50782], Loss: 6.7829\n",
            "Epoch [1/10], Step [10570/50782], Loss: 7.0183\n",
            "Epoch [1/10], Step [10580/50782], Loss: 6.4235\n",
            "Epoch [1/10], Step [10590/50782], Loss: 6.6176\n",
            "Epoch [1/10], Step [10600/50782], Loss: 7.6727\n",
            "Epoch [1/10], Step [10610/50782], Loss: 8.3361\n",
            "Epoch [1/10], Step [10620/50782], Loss: 8.5644\n",
            "Epoch [1/10], Step [10630/50782], Loss: 7.5117\n",
            "Epoch [1/10], Step [10640/50782], Loss: 8.4310\n",
            "Epoch [1/10], Step [10650/50782], Loss: 7.2766\n",
            "Epoch [1/10], Step [10660/50782], Loss: 8.8034\n",
            "Epoch [1/10], Step [10670/50782], Loss: 7.0231\n",
            "Epoch [1/10], Step [10680/50782], Loss: 8.5088\n",
            "Epoch [1/10], Step [10690/50782], Loss: 7.2929\n",
            "Epoch [1/10], Step [10700/50782], Loss: 7.9831\n",
            "Epoch [1/10], Step [10710/50782], Loss: 7.6385\n",
            "Epoch [1/10], Step [10720/50782], Loss: 7.8901\n",
            "Epoch [1/10], Step [10730/50782], Loss: 7.3917\n",
            "Epoch [1/10], Step [10740/50782], Loss: 8.4095\n",
            "Epoch [1/10], Step [10750/50782], Loss: 7.3089\n",
            "Epoch [1/10], Step [10760/50782], Loss: 6.4389\n",
            "Epoch [1/10], Step [10770/50782], Loss: 7.3016\n",
            "Epoch [1/10], Step [10780/50782], Loss: 6.3643\n",
            "Epoch [1/10], Step [10790/50782], Loss: 6.8545\n",
            "Epoch [1/10], Step [10800/50782], Loss: 6.6207\n",
            "Epoch [1/10], Step [10810/50782], Loss: 6.7384\n",
            "Epoch [1/10], Step [10820/50782], Loss: 7.3554\n",
            "Epoch [1/10], Step [10830/50782], Loss: 6.0995\n",
            "Epoch [1/10], Step [10840/50782], Loss: 7.1071\n",
            "Epoch [1/10], Step [10850/50782], Loss: 7.3318\n",
            "Epoch [1/10], Step [10860/50782], Loss: 8.8285\n",
            "Epoch [1/10], Step [10870/50782], Loss: 8.3862\n",
            "Epoch [1/10], Step [10880/50782], Loss: 7.8754\n",
            "Epoch [1/10], Step [10890/50782], Loss: 6.3633\n",
            "Epoch [1/10], Step [10900/50782], Loss: 7.6956\n",
            "Epoch [1/10], Step [10910/50782], Loss: 7.2271\n",
            "Epoch [1/10], Step [10920/50782], Loss: 7.1552\n",
            "Epoch [1/10], Step [10930/50782], Loss: 6.5381\n",
            "Epoch [1/10], Step [10940/50782], Loss: 7.3378\n",
            "Epoch [1/10], Step [10950/50782], Loss: 6.7932\n",
            "Epoch [1/10], Step [10960/50782], Loss: 7.8672\n",
            "Epoch [1/10], Step [10970/50782], Loss: 7.0923\n",
            "Epoch [1/10], Step [10980/50782], Loss: 7.4515\n",
            "Epoch [1/10], Step [10990/50782], Loss: 7.0096\n",
            "Epoch [1/10], Step [11000/50782], Loss: 8.7577\n",
            "Epoch [1/10], Step [11010/50782], Loss: 7.5552\n",
            "Epoch [1/10], Step [11020/50782], Loss: 8.8175\n",
            "Epoch [1/10], Step [11030/50782], Loss: 7.0111\n",
            "Epoch [1/10], Step [11040/50782], Loss: 8.4238\n",
            "Epoch [1/10], Step [11050/50782], Loss: 6.0236\n",
            "Epoch [1/10], Step [11060/50782], Loss: 7.7376\n",
            "Epoch [1/10], Step [11070/50782], Loss: 7.3076\n",
            "Epoch [1/10], Step [11080/50782], Loss: 8.8308\n",
            "Epoch [1/10], Step [11090/50782], Loss: 6.0879\n",
            "Epoch [1/10], Step [11100/50782], Loss: 6.8051\n",
            "Epoch [1/10], Step [11110/50782], Loss: 8.8510\n",
            "Epoch [1/10], Step [11120/50782], Loss: 8.6667\n",
            "Epoch [1/10], Step [11130/50782], Loss: 6.8401\n",
            "Epoch [1/10], Step [11140/50782], Loss: 6.8302\n",
            "Epoch [1/10], Step [11150/50782], Loss: 7.1291\n",
            "Epoch [1/10], Step [11160/50782], Loss: 7.8478\n",
            "Epoch [1/10], Step [11170/50782], Loss: 7.8755\n",
            "Epoch [1/10], Step [11180/50782], Loss: 7.0744\n",
            "Epoch [1/10], Step [11190/50782], Loss: 8.7945\n",
            "Epoch [1/10], Step [11200/50782], Loss: 5.6856\n",
            "Epoch [1/10], Step [11210/50782], Loss: 6.5608\n",
            "Epoch [1/10], Step [11220/50782], Loss: 8.7758\n",
            "Epoch [1/10], Step [11230/50782], Loss: 6.7639\n",
            "Epoch [1/10], Step [11240/50782], Loss: 6.6016\n",
            "Epoch [1/10], Step [11250/50782], Loss: 5.5944\n",
            "Epoch [1/10], Step [11260/50782], Loss: 6.4240\n",
            "Epoch [1/10], Step [11270/50782], Loss: 8.1955\n",
            "Epoch [1/10], Step [11280/50782], Loss: 7.6809\n",
            "Epoch [1/10], Step [11290/50782], Loss: 7.2023\n",
            "Epoch [1/10], Step [11300/50782], Loss: 6.6156\n",
            "Epoch [1/10], Step [11310/50782], Loss: 6.2964\n",
            "Epoch [1/10], Step [11320/50782], Loss: 6.9621\n",
            "Epoch [1/10], Step [11330/50782], Loss: 6.8206\n",
            "Epoch [1/10], Step [11340/50782], Loss: 8.0271\n",
            "Epoch [1/10], Step [11350/50782], Loss: 8.4984\n",
            "Epoch [1/10], Step [11360/50782], Loss: 6.9261\n",
            "Epoch [1/10], Step [11370/50782], Loss: 6.1580\n",
            "Epoch [1/10], Step [11380/50782], Loss: 6.9980\n",
            "Epoch [1/10], Step [11390/50782], Loss: 7.9873\n",
            "Epoch [1/10], Step [11400/50782], Loss: 8.1065\n",
            "Epoch [1/10], Step [11410/50782], Loss: 7.1911\n",
            "Epoch [1/10], Step [11420/50782], Loss: 7.6516\n",
            "Epoch [1/10], Step [11430/50782], Loss: 7.7570\n",
            "Epoch [1/10], Step [11440/50782], Loss: 7.1349\n",
            "Epoch [1/10], Step [11450/50782], Loss: 5.9131\n",
            "Epoch [1/10], Step [11460/50782], Loss: 8.8491\n",
            "Epoch [1/10], Step [11470/50782], Loss: 7.8016\n",
            "Epoch [1/10], Step [11480/50782], Loss: 6.7535\n",
            "Epoch [1/10], Step [11490/50782], Loss: 9.4083\n",
            "Epoch [1/10], Step [11500/50782], Loss: 7.0887\n",
            "Epoch [1/10], Step [11510/50782], Loss: 7.2671\n",
            "Epoch [1/10], Step [11520/50782], Loss: 7.7010\n",
            "Epoch [1/10], Step [11530/50782], Loss: 6.9006\n",
            "Epoch [1/10], Step [11540/50782], Loss: 8.2657\n",
            "Epoch [1/10], Step [11550/50782], Loss: 7.4116\n",
            "Epoch [1/10], Step [11560/50782], Loss: 6.9425\n",
            "Epoch [1/10], Step [11570/50782], Loss: 6.9743\n",
            "Epoch [1/10], Step [11580/50782], Loss: 7.0534\n",
            "Epoch [1/10], Step [11590/50782], Loss: 7.1631\n",
            "Epoch [1/10], Step [11600/50782], Loss: 5.8550\n",
            "Epoch [1/10], Step [11610/50782], Loss: 6.9908\n",
            "Epoch [1/10], Step [11620/50782], Loss: 8.1851\n",
            "Epoch [1/10], Step [11630/50782], Loss: 8.5618\n",
            "Epoch [1/10], Step [11640/50782], Loss: 7.7473\n",
            "Epoch [1/10], Step [11650/50782], Loss: 8.1411\n",
            "Epoch [1/10], Step [11660/50782], Loss: 7.6395\n",
            "Epoch [1/10], Step [11670/50782], Loss: 7.4656\n",
            "Epoch [1/10], Step [11680/50782], Loss: 6.4215\n",
            "Epoch [1/10], Step [11690/50782], Loss: 7.2834\n",
            "Epoch [1/10], Step [11700/50782], Loss: 7.6720\n",
            "Epoch [1/10], Step [11710/50782], Loss: 7.2207\n",
            "Epoch [1/10], Step [11720/50782], Loss: 8.0666\n",
            "Epoch [1/10], Step [11730/50782], Loss: 9.0457\n",
            "Epoch [1/10], Step [11740/50782], Loss: 6.9707\n",
            "Epoch [1/10], Step [11750/50782], Loss: 6.3947\n",
            "Epoch [1/10], Step [11760/50782], Loss: 7.1841\n",
            "Epoch [1/10], Step [11770/50782], Loss: 6.7895\n",
            "Epoch [1/10], Step [11780/50782], Loss: 6.0529\n",
            "Epoch [1/10], Step [11790/50782], Loss: 7.0238\n",
            "Epoch [1/10], Step [11800/50782], Loss: 8.9078\n",
            "Epoch [1/10], Step [11810/50782], Loss: 7.3743\n",
            "Epoch [1/10], Step [11820/50782], Loss: 6.9036\n",
            "Epoch [1/10], Step [11830/50782], Loss: 6.1110\n",
            "Epoch [1/10], Step [11840/50782], Loss: 5.6315\n",
            "Epoch [1/10], Step [11850/50782], Loss: 7.1798\n",
            "Epoch [1/10], Step [11860/50782], Loss: 9.2575\n",
            "Epoch [1/10], Step [11870/50782], Loss: 6.6854\n",
            "Epoch [1/10], Step [11880/50782], Loss: 6.4008\n",
            "Epoch [1/10], Step [11890/50782], Loss: 6.1583\n",
            "Epoch [1/10], Step [11900/50782], Loss: 7.3444\n",
            "Epoch [1/10], Step [11910/50782], Loss: 7.4484\n",
            "Epoch [1/10], Step [11920/50782], Loss: 6.5666\n",
            "Epoch [1/10], Step [11930/50782], Loss: 7.6860\n",
            "Epoch [1/10], Step [11940/50782], Loss: 7.0816\n",
            "Epoch [1/10], Step [11950/50782], Loss: 9.3666\n",
            "Epoch [1/10], Step [11960/50782], Loss: 7.6634\n",
            "Epoch [1/10], Step [11970/50782], Loss: 7.8263\n",
            "Epoch [1/10], Step [11980/50782], Loss: 6.5187\n",
            "Epoch [1/10], Step [11990/50782], Loss: 6.8997\n",
            "Epoch [1/10], Step [12000/50782], Loss: 7.4879\n",
            "Epoch [1/10], Step [12010/50782], Loss: 6.9097\n",
            "Epoch [1/10], Step [12020/50782], Loss: 8.2093\n",
            "Epoch [1/10], Step [12030/50782], Loss: 6.1832\n",
            "Epoch [1/10], Step [12040/50782], Loss: 7.0246\n",
            "Epoch [1/10], Step [12050/50782], Loss: 6.1708\n",
            "Epoch [1/10], Step [12060/50782], Loss: 7.2882\n",
            "Epoch [1/10], Step [12070/50782], Loss: 7.7028\n",
            "Epoch [1/10], Step [12080/50782], Loss: 6.9477\n",
            "Epoch [1/10], Step [12090/50782], Loss: 8.3859\n",
            "Epoch [1/10], Step [12100/50782], Loss: 6.8999\n",
            "Epoch [1/10], Step [12110/50782], Loss: 6.3868\n",
            "Epoch [1/10], Step [12120/50782], Loss: 6.8523\n",
            "Epoch [1/10], Step [12130/50782], Loss: 6.8852\n",
            "Epoch [1/10], Step [12140/50782], Loss: 7.1061\n",
            "Epoch [1/10], Step [12150/50782], Loss: 7.0319\n",
            "Epoch [1/10], Step [12160/50782], Loss: 7.2097\n",
            "Epoch [1/10], Step [12170/50782], Loss: 7.5036\n",
            "Epoch [1/10], Step [12180/50782], Loss: 6.8846\n",
            "Epoch [1/10], Step [12190/50782], Loss: 8.9177\n",
            "Epoch [1/10], Step [12200/50782], Loss: 9.1419\n",
            "Epoch [1/10], Step [12210/50782], Loss: 6.3568\n",
            "Epoch [1/10], Step [12220/50782], Loss: 6.3944\n",
            "Epoch [1/10], Step [12230/50782], Loss: 7.7203\n",
            "Epoch [1/10], Step [12240/50782], Loss: 7.1764\n",
            "Epoch [1/10], Step [12250/50782], Loss: 7.5671\n",
            "Epoch [1/10], Step [12260/50782], Loss: 8.5598\n",
            "Epoch [1/10], Step [12270/50782], Loss: 8.0985\n",
            "Epoch [1/10], Step [12280/50782], Loss: 7.4055\n",
            "Epoch [1/10], Step [12290/50782], Loss: 8.1748\n",
            "Epoch [1/10], Step [12300/50782], Loss: 7.3968\n",
            "Epoch [1/10], Step [12310/50782], Loss: 6.4872\n",
            "Epoch [1/10], Step [12320/50782], Loss: 6.6082\n",
            "Epoch [1/10], Step [12330/50782], Loss: 6.5258\n",
            "Epoch [1/10], Step [12340/50782], Loss: 7.3362\n",
            "Epoch [1/10], Step [12350/50782], Loss: 7.8910\n",
            "Epoch [1/10], Step [12360/50782], Loss: 8.1512\n",
            "Epoch [1/10], Step [12370/50782], Loss: 6.7197\n",
            "Epoch [1/10], Step [12380/50782], Loss: 6.9887\n",
            "Epoch [1/10], Step [12390/50782], Loss: 8.3069\n",
            "Epoch [1/10], Step [12400/50782], Loss: 9.0685\n",
            "Epoch [1/10], Step [12410/50782], Loss: 7.1056\n",
            "Epoch [1/10], Step [12420/50782], Loss: 7.5113\n",
            "Epoch [1/10], Step [12430/50782], Loss: 6.2918\n",
            "Epoch [1/10], Step [12440/50782], Loss: 6.6694\n",
            "Epoch [1/10], Step [12450/50782], Loss: 6.8444\n",
            "Epoch [1/10], Step [12460/50782], Loss: 6.5518\n",
            "Epoch [1/10], Step [12470/50782], Loss: 7.5456\n",
            "Epoch [1/10], Step [12480/50782], Loss: 7.9484\n",
            "Epoch [1/10], Step [12490/50782], Loss: 7.7453\n",
            "Epoch [1/10], Step [12500/50782], Loss: 6.5204\n",
            "Epoch [1/10], Step [12510/50782], Loss: 6.6569\n",
            "Epoch [1/10], Step [12520/50782], Loss: 7.1409\n",
            "Epoch [1/10], Step [12530/50782], Loss: 8.8572\n",
            "Epoch [1/10], Step [12540/50782], Loss: 7.7284\n",
            "Epoch [1/10], Step [12550/50782], Loss: 8.3247\n",
            "Epoch [1/10], Step [12560/50782], Loss: 6.7783\n",
            "Epoch [1/10], Step [12570/50782], Loss: 6.7163\n",
            "Epoch [1/10], Step [12580/50782], Loss: 7.7145\n",
            "Epoch [1/10], Step [12590/50782], Loss: 6.2884\n",
            "Epoch [1/10], Step [12600/50782], Loss: 6.9922\n",
            "Epoch [1/10], Step [12610/50782], Loss: 7.2001\n",
            "Epoch [1/10], Step [12620/50782], Loss: 6.8957\n",
            "Epoch [1/10], Step [12630/50782], Loss: 7.8591\n",
            "Epoch [1/10], Step [12640/50782], Loss: 7.3544\n",
            "Epoch [1/10], Step [12650/50782], Loss: 6.5721\n",
            "Epoch [1/10], Step [12660/50782], Loss: 6.6522\n",
            "Epoch [1/10], Step [12670/50782], Loss: 7.2589\n",
            "Epoch [1/10], Step [12680/50782], Loss: 7.4632\n",
            "Epoch [1/10], Step [12690/50782], Loss: 6.8687\n",
            "Epoch [1/10], Step [12700/50782], Loss: 7.8139\n",
            "Epoch [1/10], Step [12710/50782], Loss: 6.8296\n",
            "Epoch [1/10], Step [12720/50782], Loss: 7.1114\n",
            "Epoch [1/10], Step [12730/50782], Loss: 7.7975\n",
            "Epoch [1/10], Step [12740/50782], Loss: 6.6479\n",
            "Epoch [1/10], Step [12750/50782], Loss: 8.8843\n",
            "Epoch [1/10], Step [12760/50782], Loss: 7.8765\n",
            "Epoch [1/10], Step [12770/50782], Loss: 6.7636\n",
            "Epoch [1/10], Step [12780/50782], Loss: 7.2597\n",
            "Epoch [1/10], Step [12790/50782], Loss: 6.4354\n",
            "Epoch [1/10], Step [12800/50782], Loss: 7.9573\n",
            "Epoch [1/10], Step [12810/50782], Loss: 6.7951\n",
            "Epoch [1/10], Step [12820/50782], Loss: 6.0418\n",
            "Epoch [1/10], Step [12830/50782], Loss: 8.6621\n",
            "Epoch [1/10], Step [12840/50782], Loss: 6.9561\n",
            "Epoch [1/10], Step [12850/50782], Loss: 7.0944\n",
            "Epoch [1/10], Step [12860/50782], Loss: 8.3305\n",
            "Epoch [1/10], Step [12870/50782], Loss: 6.9953\n",
            "Epoch [1/10], Step [12880/50782], Loss: 7.3025\n",
            "Epoch [1/10], Step [12890/50782], Loss: 8.4607\n",
            "Epoch [1/10], Step [12900/50782], Loss: 6.7852\n",
            "Epoch [1/10], Step [12910/50782], Loss: 7.2087\n",
            "Epoch [1/10], Step [12920/50782], Loss: 9.9838\n",
            "Epoch [1/10], Step [12930/50782], Loss: 5.8710\n",
            "Epoch [1/10], Step [12940/50782], Loss: 8.5563\n",
            "Epoch [1/10], Step [12950/50782], Loss: 7.9362\n",
            "Epoch [1/10], Step [12960/50782], Loss: 8.5009\n",
            "Epoch [1/10], Step [12970/50782], Loss: 6.0845\n",
            "Epoch [1/10], Step [12980/50782], Loss: 7.0625\n",
            "Epoch [1/10], Step [12990/50782], Loss: 8.4869\n",
            "Epoch [1/10], Step [13000/50782], Loss: 7.1374\n",
            "Epoch [1/10], Step [13010/50782], Loss: 8.6637\n",
            "Epoch [1/10], Step [13020/50782], Loss: 6.6874\n",
            "Epoch [1/10], Step [13030/50782], Loss: 7.3530\n",
            "Epoch [1/10], Step [13040/50782], Loss: 7.5449\n",
            "Epoch [1/10], Step [13050/50782], Loss: 7.7136\n",
            "Epoch [1/10], Step [13060/50782], Loss: 6.2628\n",
            "Epoch [1/10], Step [13070/50782], Loss: 8.2031\n",
            "Epoch [1/10], Step [13080/50782], Loss: 8.7487\n",
            "Epoch [1/10], Step [13090/50782], Loss: 9.2961\n",
            "Epoch [1/10], Step [13100/50782], Loss: 6.8168\n",
            "Epoch [1/10], Step [13110/50782], Loss: 7.7396\n",
            "Epoch [1/10], Step [13120/50782], Loss: 8.3763\n",
            "Epoch [1/10], Step [13130/50782], Loss: 6.7063\n",
            "Epoch [1/10], Step [13140/50782], Loss: 6.1778\n",
            "Epoch [1/10], Step [13150/50782], Loss: 6.3773\n",
            "Epoch [1/10], Step [13160/50782], Loss: 7.1437\n",
            "Epoch [1/10], Step [13170/50782], Loss: 6.3893\n",
            "Epoch [1/10], Step [13180/50782], Loss: 8.2068\n",
            "Epoch [1/10], Step [13190/50782], Loss: 8.3684\n",
            "Epoch [1/10], Step [13200/50782], Loss: 6.7927\n",
            "Epoch [1/10], Step [13210/50782], Loss: 8.1051\n",
            "Epoch [1/10], Step [13220/50782], Loss: 6.9301\n",
            "Epoch [1/10], Step [13230/50782], Loss: 6.3056\n",
            "Epoch [1/10], Step [13240/50782], Loss: 8.7616\n",
            "Epoch [1/10], Step [13250/50782], Loss: 7.6320\n",
            "Epoch [1/10], Step [13260/50782], Loss: 7.6676\n",
            "Epoch [1/10], Step [13270/50782], Loss: 8.4784\n",
            "Epoch [1/10], Step [13280/50782], Loss: 8.5913\n",
            "Epoch [1/10], Step [13290/50782], Loss: 7.0764\n",
            "Epoch [1/10], Step [13300/50782], Loss: 6.5343\n",
            "Epoch [1/10], Step [13310/50782], Loss: 8.3611\n",
            "Epoch [1/10], Step [13320/50782], Loss: 5.9638\n",
            "Epoch [1/10], Step [13330/50782], Loss: 9.6631\n",
            "Epoch [1/10], Step [13340/50782], Loss: 9.5410\n",
            "Epoch [1/10], Step [13350/50782], Loss: 7.8676\n",
            "Epoch [1/10], Step [13360/50782], Loss: 7.4903\n",
            "Epoch [1/10], Step [13370/50782], Loss: 7.6325\n",
            "Epoch [1/10], Step [13380/50782], Loss: 7.2555\n",
            "Epoch [1/10], Step [13390/50782], Loss: 7.9068\n",
            "Epoch [1/10], Step [13400/50782], Loss: 8.8321\n",
            "Epoch [1/10], Step [13410/50782], Loss: 7.0493\n",
            "Epoch [1/10], Step [13420/50782], Loss: 6.6081\n",
            "Epoch [1/10], Step [13430/50782], Loss: 7.8791\n",
            "Epoch [1/10], Step [13440/50782], Loss: 7.3779\n",
            "Epoch [1/10], Step [13450/50782], Loss: 6.9581\n",
            "Epoch [1/10], Step [13460/50782], Loss: 7.4504\n",
            "Epoch [1/10], Step [13470/50782], Loss: 5.3809\n",
            "Epoch [1/10], Step [13480/50782], Loss: 7.9558\n",
            "Epoch [1/10], Step [13490/50782], Loss: 7.2216\n",
            "Epoch [1/10], Step [13500/50782], Loss: 8.5866\n",
            "Epoch [1/10], Step [13510/50782], Loss: 7.4572\n",
            "Epoch [1/10], Step [13520/50782], Loss: 7.5569\n",
            "Epoch [1/10], Step [13530/50782], Loss: 7.0996\n",
            "Epoch [1/10], Step [13540/50782], Loss: 9.1259\n",
            "Epoch [1/10], Step [13550/50782], Loss: 6.7282\n",
            "Epoch [1/10], Step [13560/50782], Loss: 7.2987\n",
            "Epoch [1/10], Step [13570/50782], Loss: 8.8430\n",
            "Epoch [1/10], Step [13580/50782], Loss: 7.1076\n",
            "Epoch [1/10], Step [13590/50782], Loss: 7.2085\n",
            "Epoch [1/10], Step [13600/50782], Loss: 7.0498\n",
            "Epoch [1/10], Step [13610/50782], Loss: 6.1374\n",
            "Epoch [1/10], Step [13620/50782], Loss: 7.3240\n",
            "Epoch [1/10], Step [13630/50782], Loss: 7.8857\n",
            "Epoch [1/10], Step [13640/50782], Loss: 6.0809\n",
            "Epoch [1/10], Step [13650/50782], Loss: 8.9384\n",
            "Epoch [1/10], Step [13660/50782], Loss: 7.9170\n",
            "Epoch [1/10], Step [13670/50782], Loss: 6.9599\n",
            "Epoch [1/10], Step [13680/50782], Loss: 7.6867\n",
            "Epoch [1/10], Step [13690/50782], Loss: 8.0709\n",
            "Epoch [1/10], Step [13700/50782], Loss: 7.0077\n",
            "Epoch [1/10], Step [13710/50782], Loss: 6.7947\n",
            "Epoch [1/10], Step [13720/50782], Loss: 7.1943\n",
            "Epoch [1/10], Step [13730/50782], Loss: 6.3121\n",
            "Epoch [1/10], Step [13740/50782], Loss: 7.9875\n",
            "Epoch [1/10], Step [13750/50782], Loss: 6.4161\n",
            "Epoch [1/10], Step [13760/50782], Loss: 7.6286\n",
            "Epoch [1/10], Step [13770/50782], Loss: 8.4191\n",
            "Epoch [1/10], Step [13780/50782], Loss: 7.9899\n",
            "Epoch [1/10], Step [13790/50782], Loss: 7.3912\n",
            "Epoch [1/10], Step [13800/50782], Loss: 6.3913\n",
            "Epoch [1/10], Step [13810/50782], Loss: 6.9447\n",
            "Epoch [1/10], Step [13820/50782], Loss: 7.5485\n",
            "Epoch [1/10], Step [13830/50782], Loss: 7.4050\n",
            "Epoch [1/10], Step [13840/50782], Loss: 8.7770\n",
            "Epoch [1/10], Step [13850/50782], Loss: 7.3592\n",
            "Epoch [1/10], Step [13860/50782], Loss: 7.8156\n",
            "Epoch [1/10], Step [13870/50782], Loss: 8.2762\n",
            "Epoch [1/10], Step [13880/50782], Loss: 8.2001\n",
            "Epoch [1/10], Step [13890/50782], Loss: 8.1982\n",
            "Epoch [1/10], Step [13900/50782], Loss: 7.4981\n",
            "Epoch [1/10], Step [13910/50782], Loss: 8.8207\n",
            "Epoch [1/10], Step [13920/50782], Loss: 7.0536\n",
            "Epoch [1/10], Step [13930/50782], Loss: 8.0334\n",
            "Epoch [1/10], Step [13940/50782], Loss: 6.8115\n",
            "Epoch [1/10], Step [13950/50782], Loss: 5.0171\n",
            "Epoch [1/10], Step [13960/50782], Loss: 8.3123\n",
            "Epoch [1/10], Step [13970/50782], Loss: 7.2422\n",
            "Epoch [1/10], Step [13980/50782], Loss: 6.7322\n",
            "Epoch [1/10], Step [13990/50782], Loss: 5.2610\n",
            "Epoch [1/10], Step [14000/50782], Loss: 8.8811\n",
            "Epoch [1/10], Step [14010/50782], Loss: 7.9746\n",
            "Epoch [1/10], Step [14020/50782], Loss: 5.4245\n",
            "Epoch [1/10], Step [14030/50782], Loss: 8.1810\n",
            "Epoch [1/10], Step [14040/50782], Loss: 6.8375\n",
            "Epoch [1/10], Step [14050/50782], Loss: 8.6619\n",
            "Epoch [1/10], Step [14060/50782], Loss: 8.9053\n",
            "Epoch [1/10], Step [14070/50782], Loss: 5.9484\n",
            "Epoch [1/10], Step [14080/50782], Loss: 7.3638\n",
            "Epoch [1/10], Step [14090/50782], Loss: 8.3588\n",
            "Epoch [1/10], Step [14100/50782], Loss: 6.2216\n",
            "Epoch [1/10], Step [14110/50782], Loss: 7.7776\n",
            "Epoch [1/10], Step [14120/50782], Loss: 6.2944\n",
            "Epoch [1/10], Step [14130/50782], Loss: 7.8803\n",
            "Epoch [1/10], Step [14140/50782], Loss: 6.8768\n",
            "Epoch [1/10], Step [14150/50782], Loss: 7.2452\n",
            "Epoch [1/10], Step [14160/50782], Loss: 8.0397\n",
            "Epoch [1/10], Step [14170/50782], Loss: 7.0795\n",
            "Epoch [1/10], Step [14180/50782], Loss: 6.1195\n",
            "Epoch [1/10], Step [14190/50782], Loss: 6.4858\n",
            "Epoch [1/10], Step [14200/50782], Loss: 7.8082\n",
            "Epoch [1/10], Step [14210/50782], Loss: 6.9057\n",
            "Epoch [1/10], Step [14220/50782], Loss: 7.4580\n",
            "Epoch [1/10], Step [14230/50782], Loss: 7.6559\n",
            "Epoch [1/10], Step [14240/50782], Loss: 7.1646\n",
            "Epoch [1/10], Step [14250/50782], Loss: 9.0180\n",
            "Epoch [1/10], Step [14260/50782], Loss: 6.2268\n",
            "Epoch [1/10], Step [14270/50782], Loss: 8.7245\n",
            "Epoch [1/10], Step [14280/50782], Loss: 7.7789\n",
            "Epoch [1/10], Step [14290/50782], Loss: 6.3707\n",
            "Epoch [1/10], Step [14300/50782], Loss: 7.2741\n",
            "Epoch [1/10], Step [14310/50782], Loss: 5.6320\n",
            "Epoch [1/10], Step [14320/50782], Loss: 7.5111\n",
            "Epoch [1/10], Step [14330/50782], Loss: 8.4944\n",
            "Epoch [1/10], Step [14340/50782], Loss: 5.4024\n",
            "Epoch [1/10], Step [14350/50782], Loss: 8.4873\n",
            "Epoch [1/10], Step [14360/50782], Loss: 7.0632\n",
            "Epoch [1/10], Step [14370/50782], Loss: 8.2712\n",
            "Epoch [1/10], Step [14380/50782], Loss: 5.4686\n",
            "Epoch [1/10], Step [14390/50782], Loss: 7.5881\n",
            "Epoch [1/10], Step [14400/50782], Loss: 7.4198\n",
            "Epoch [1/10], Step [14410/50782], Loss: 7.1765\n",
            "Epoch [1/10], Step [14420/50782], Loss: 7.3221\n",
            "Epoch [1/10], Step [14430/50782], Loss: 8.6691\n",
            "Epoch [1/10], Step [14440/50782], Loss: 5.8864\n",
            "Epoch [1/10], Step [14450/50782], Loss: 5.7089\n",
            "Epoch [1/10], Step [14460/50782], Loss: 8.5946\n",
            "Epoch [1/10], Step [14470/50782], Loss: 7.2073\n",
            "Epoch [1/10], Step [14480/50782], Loss: 7.9738\n",
            "Epoch [1/10], Step [14490/50782], Loss: 7.2991\n",
            "Epoch [1/10], Step [14500/50782], Loss: 8.0351\n",
            "Epoch [1/10], Step [14510/50782], Loss: 6.0830\n",
            "Epoch [1/10], Step [14520/50782], Loss: 7.0179\n",
            "Epoch [1/10], Step [14530/50782], Loss: 7.4789\n",
            "Epoch [1/10], Step [14540/50782], Loss: 6.9425\n",
            "Epoch [1/10], Step [14550/50782], Loss: 8.4108\n",
            "Epoch [1/10], Step [14560/50782], Loss: 9.7985\n",
            "Epoch [1/10], Step [14570/50782], Loss: 8.1165\n",
            "Epoch [1/10], Step [14580/50782], Loss: 6.9117\n",
            "Epoch [1/10], Step [14590/50782], Loss: 6.5222\n",
            "Epoch [1/10], Step [14600/50782], Loss: 7.8879\n",
            "Epoch [1/10], Step [14610/50782], Loss: 7.7933\n",
            "Epoch [1/10], Step [14620/50782], Loss: 8.1777\n",
            "Epoch [1/10], Step [14630/50782], Loss: 7.4611\n",
            "Epoch [1/10], Step [14640/50782], Loss: 7.2907\n",
            "Epoch [1/10], Step [14650/50782], Loss: 8.1005\n",
            "Epoch [1/10], Step [14660/50782], Loss: 7.0433\n",
            "Epoch [1/10], Step [14670/50782], Loss: 9.1133\n",
            "Epoch [1/10], Step [14680/50782], Loss: 5.9696\n",
            "Epoch [1/10], Step [14690/50782], Loss: 7.8611\n",
            "Epoch [1/10], Step [14700/50782], Loss: 6.3193\n",
            "Epoch [1/10], Step [14710/50782], Loss: 6.9056\n",
            "Epoch [1/10], Step [14720/50782], Loss: 6.2917\n",
            "Epoch [1/10], Step [14730/50782], Loss: 8.8489\n",
            "Epoch [1/10], Step [14740/50782], Loss: 6.3694\n",
            "Epoch [1/10], Step [14750/50782], Loss: 6.3998\n",
            "Epoch [1/10], Step [14760/50782], Loss: 6.6639\n",
            "Epoch [1/10], Step [14770/50782], Loss: 7.4605\n",
            "Epoch [1/10], Step [14780/50782], Loss: 9.7712\n",
            "Epoch [1/10], Step [14790/50782], Loss: 6.1999\n",
            "Epoch [1/10], Step [14800/50782], Loss: 7.2047\n",
            "Epoch [1/10], Step [14810/50782], Loss: 6.7324\n",
            "Epoch [1/10], Step [14820/50782], Loss: 5.2490\n",
            "Epoch [1/10], Step [14830/50782], Loss: 6.7691\n",
            "Epoch [1/10], Step [14840/50782], Loss: 7.6952\n",
            "Epoch [1/10], Step [14850/50782], Loss: 9.2596\n",
            "Epoch [1/10], Step [14860/50782], Loss: 5.7990\n",
            "Epoch [1/10], Step [14870/50782], Loss: 8.5708\n",
            "Epoch [1/10], Step [14880/50782], Loss: 6.7533\n",
            "Epoch [1/10], Step [14890/50782], Loss: 7.8386\n",
            "Epoch [1/10], Step [14900/50782], Loss: 7.4486\n",
            "Epoch [1/10], Step [14910/50782], Loss: 7.4080\n",
            "Epoch [1/10], Step [14920/50782], Loss: 8.7132\n",
            "Epoch [1/10], Step [14930/50782], Loss: 7.9801\n",
            "Epoch [1/10], Step [14940/50782], Loss: 6.9669\n",
            "Epoch [1/10], Step [14950/50782], Loss: 7.7127\n",
            "Epoch [1/10], Step [14960/50782], Loss: 9.0450\n",
            "Epoch [1/10], Step [14970/50782], Loss: 7.1654\n",
            "Epoch [1/10], Step [14980/50782], Loss: 6.3671\n",
            "Epoch [1/10], Step [14990/50782], Loss: 4.6026\n",
            "Epoch [1/10], Step [15000/50782], Loss: 6.8180\n",
            "Epoch [1/10], Step [15010/50782], Loss: 5.9267\n",
            "Epoch [1/10], Step [15020/50782], Loss: 8.3269\n",
            "Epoch [1/10], Step [15030/50782], Loss: 6.5084\n",
            "Epoch [1/10], Step [15040/50782], Loss: 6.0682\n",
            "Epoch [1/10], Step [15050/50782], Loss: 8.0315\n",
            "Epoch [1/10], Step [15060/50782], Loss: 7.2989\n",
            "Epoch [1/10], Step [15070/50782], Loss: 7.1535\n",
            "Epoch [1/10], Step [15080/50782], Loss: 6.9855\n",
            "Epoch [1/10], Step [15090/50782], Loss: 7.3322\n",
            "Epoch [1/10], Step [15100/50782], Loss: 6.8645\n",
            "Epoch [1/10], Step [15110/50782], Loss: 7.1590\n",
            "Epoch [1/10], Step [15120/50782], Loss: 8.0479\n",
            "Epoch [1/10], Step [15130/50782], Loss: 7.0485\n",
            "Epoch [1/10], Step [15140/50782], Loss: 5.7334\n",
            "Epoch [1/10], Step [15150/50782], Loss: 6.7084\n",
            "Epoch [1/10], Step [15160/50782], Loss: 7.5465\n",
            "Epoch [1/10], Step [15170/50782], Loss: 8.2379\n",
            "Epoch [1/10], Step [15180/50782], Loss: 7.6209\n",
            "Epoch [1/10], Step [15190/50782], Loss: 6.6912\n",
            "Epoch [1/10], Step [15200/50782], Loss: 8.2741\n",
            "Epoch [1/10], Step [15210/50782], Loss: 9.1351\n",
            "Epoch [1/10], Step [15220/50782], Loss: 6.2750\n",
            "Epoch [1/10], Step [15230/50782], Loss: 8.6717\n",
            "Epoch [1/10], Step [15240/50782], Loss: 7.4913\n",
            "Epoch [1/10], Step [15250/50782], Loss: 7.8403\n",
            "Epoch [1/10], Step [15260/50782], Loss: 7.3112\n",
            "Epoch [1/10], Step [15270/50782], Loss: 8.2955\n",
            "Epoch [1/10], Step [15280/50782], Loss: 7.3060\n",
            "Epoch [1/10], Step [15290/50782], Loss: 7.6488\n",
            "Epoch [1/10], Step [15300/50782], Loss: 7.5048\n",
            "Epoch [1/10], Step [15310/50782], Loss: 6.7309\n",
            "Epoch [1/10], Step [15320/50782], Loss: 8.1334\n",
            "Epoch [1/10], Step [15330/50782], Loss: 6.9496\n",
            "Epoch [1/10], Step [15340/50782], Loss: 8.5852\n",
            "Epoch [1/10], Step [15350/50782], Loss: 6.2816\n",
            "Epoch [1/10], Step [15360/50782], Loss: 7.7460\n",
            "Epoch [1/10], Step [15370/50782], Loss: 7.4980\n",
            "Epoch [1/10], Step [15380/50782], Loss: 6.0529\n",
            "Epoch [1/10], Step [15390/50782], Loss: 7.9382\n",
            "Epoch [1/10], Step [15400/50782], Loss: 8.5652\n",
            "Epoch [1/10], Step [15410/50782], Loss: 7.8613\n",
            "Epoch [1/10], Step [15420/50782], Loss: 6.6169\n",
            "Epoch [1/10], Step [15430/50782], Loss: 7.8813\n",
            "Epoch [1/10], Step [15440/50782], Loss: 8.6177\n",
            "Epoch [1/10], Step [15450/50782], Loss: 6.9026\n",
            "Epoch [1/10], Step [15460/50782], Loss: 6.9138\n",
            "Epoch [1/10], Step [15470/50782], Loss: 7.1378\n",
            "Epoch [1/10], Step [15480/50782], Loss: 7.2728\n",
            "Epoch [1/10], Step [15490/50782], Loss: 8.7143\n",
            "Epoch [1/10], Step [15500/50782], Loss: 10.2248\n",
            "Epoch [1/10], Step [15510/50782], Loss: 7.0516\n",
            "Epoch [1/10], Step [15520/50782], Loss: 7.1587\n",
            "Epoch [1/10], Step [15530/50782], Loss: 6.9153\n",
            "Epoch [1/10], Step [15540/50782], Loss: 6.7475\n",
            "Epoch [1/10], Step [15550/50782], Loss: 7.2737\n",
            "Epoch [1/10], Step [15560/50782], Loss: 5.9550\n",
            "Epoch [1/10], Step [15570/50782], Loss: 7.3324\n",
            "Epoch [1/10], Step [15580/50782], Loss: 7.4026\n",
            "Epoch [1/10], Step [15590/50782], Loss: 6.6412\n",
            "Epoch [1/10], Step [15600/50782], Loss: 6.7450\n",
            "Epoch [1/10], Step [15610/50782], Loss: 7.0786\n",
            "Epoch [1/10], Step [15620/50782], Loss: 7.6341\n",
            "Epoch [1/10], Step [15630/50782], Loss: 7.6329\n",
            "Epoch [1/10], Step [15640/50782], Loss: 7.3403\n",
            "Epoch [1/10], Step [15650/50782], Loss: 6.3589\n",
            "Epoch [1/10], Step [15660/50782], Loss: 6.6651\n",
            "Epoch [1/10], Step [15670/50782], Loss: 6.6177\n",
            "Epoch [1/10], Step [15680/50782], Loss: 7.8370\n",
            "Epoch [1/10], Step [15690/50782], Loss: 6.4808\n",
            "Epoch [1/10], Step [15700/50782], Loss: 7.0508\n",
            "Epoch [1/10], Step [15710/50782], Loss: 8.2703\n",
            "Epoch [1/10], Step [15720/50782], Loss: 8.7636\n",
            "Epoch [1/10], Step [15730/50782], Loss: 6.8195\n",
            "Epoch [1/10], Step [15740/50782], Loss: 6.9125\n",
            "Epoch [1/10], Step [15750/50782], Loss: 7.1381\n",
            "Epoch [1/10], Step [15760/50782], Loss: 7.5922\n",
            "Epoch [1/10], Step [15770/50782], Loss: 7.7279\n",
            "Epoch [1/10], Step [15780/50782], Loss: 8.5680\n",
            "Epoch [1/10], Step [15790/50782], Loss: 8.5523\n",
            "Epoch [1/10], Step [15800/50782], Loss: 9.0542\n",
            "Epoch [1/10], Step [15810/50782], Loss: 7.6535\n",
            "Epoch [1/10], Step [15820/50782], Loss: 8.6235\n",
            "Epoch [1/10], Step [15830/50782], Loss: 8.3876\n",
            "Epoch [1/10], Step [15840/50782], Loss: 8.1067\n",
            "Epoch [1/10], Step [15850/50782], Loss: 6.7497\n",
            "Epoch [1/10], Step [15860/50782], Loss: 7.6195\n",
            "Epoch [1/10], Step [15870/50782], Loss: 7.0319\n",
            "Epoch [1/10], Step [15880/50782], Loss: 6.2884\n",
            "Epoch [1/10], Step [15890/50782], Loss: 6.7173\n",
            "Epoch [1/10], Step [15900/50782], Loss: 7.0452\n",
            "Epoch [1/10], Step [15910/50782], Loss: 7.4441\n",
            "Epoch [1/10], Step [15920/50782], Loss: 6.6305\n",
            "Epoch [1/10], Step [15930/50782], Loss: 6.3295\n",
            "Epoch [1/10], Step [15940/50782], Loss: 8.6810\n",
            "Epoch [1/10], Step [15950/50782], Loss: 7.2071\n",
            "Epoch [1/10], Step [15960/50782], Loss: 6.9259\n",
            "Epoch [1/10], Step [15970/50782], Loss: 8.2852\n",
            "Epoch [1/10], Step [15980/50782], Loss: 8.8453\n",
            "Epoch [1/10], Step [15990/50782], Loss: 8.3691\n",
            "Epoch [1/10], Step [16000/50782], Loss: 7.3067\n",
            "Epoch [1/10], Step [16010/50782], Loss: 7.3450\n",
            "Epoch [1/10], Step [16020/50782], Loss: 7.5529\n",
            "Epoch [1/10], Step [16030/50782], Loss: 6.9780\n",
            "Epoch [1/10], Step [16040/50782], Loss: 8.0771\n",
            "Epoch [1/10], Step [16050/50782], Loss: 6.5256\n",
            "Epoch [1/10], Step [16060/50782], Loss: 8.2653\n",
            "Epoch [1/10], Step [16070/50782], Loss: 8.5204\n",
            "Epoch [1/10], Step [16080/50782], Loss: 8.2282\n",
            "Epoch [1/10], Step [16090/50782], Loss: 7.3501\n",
            "Epoch [1/10], Step [16100/50782], Loss: 7.7671\n",
            "Epoch [1/10], Step [16110/50782], Loss: 7.8491\n",
            "Epoch [1/10], Step [16120/50782], Loss: 5.5169\n",
            "Epoch [1/10], Step [16130/50782], Loss: 5.3373\n",
            "Epoch [1/10], Step [16140/50782], Loss: 6.7630\n",
            "Epoch [1/10], Step [16150/50782], Loss: 7.9049\n",
            "Epoch [1/10], Step [16160/50782], Loss: 6.6611\n",
            "Epoch [1/10], Step [16170/50782], Loss: 7.7490\n",
            "Epoch [1/10], Step [16180/50782], Loss: 5.8076\n",
            "Epoch [1/10], Step [16190/50782], Loss: 5.7153\n",
            "Epoch [1/10], Step [16200/50782], Loss: 7.1663\n",
            "Epoch [1/10], Step [16210/50782], Loss: 6.9836\n",
            "Epoch [1/10], Step [16220/50782], Loss: 7.7181\n",
            "Epoch [1/10], Step [16230/50782], Loss: 6.6759\n",
            "Epoch [1/10], Step [16240/50782], Loss: 7.3582\n",
            "Epoch [1/10], Step [16250/50782], Loss: 7.4504\n",
            "Epoch [1/10], Step [16260/50782], Loss: 7.3361\n",
            "Epoch [1/10], Step [16270/50782], Loss: 5.6038\n",
            "Epoch [1/10], Step [16280/50782], Loss: 8.1840\n",
            "Epoch [1/10], Step [16290/50782], Loss: 5.9613\n",
            "Epoch [1/10], Step [16300/50782], Loss: 6.5348\n",
            "Epoch [1/10], Step [16310/50782], Loss: 6.6215\n",
            "Epoch [1/10], Step [16320/50782], Loss: 7.2463\n",
            "Epoch [1/10], Step [16330/50782], Loss: 6.0500\n",
            "Epoch [1/10], Step [16340/50782], Loss: 7.6751\n",
            "Epoch [1/10], Step [16350/50782], Loss: 8.4947\n",
            "Epoch [1/10], Step [16360/50782], Loss: 6.1659\n",
            "Epoch [1/10], Step [16370/50782], Loss: 7.3100\n",
            "Epoch [1/10], Step [16380/50782], Loss: 8.2169\n",
            "Epoch [1/10], Step [16390/50782], Loss: 7.5005\n",
            "Epoch [1/10], Step [16400/50782], Loss: 8.0219\n",
            "Epoch [1/10], Step [16410/50782], Loss: 7.2901\n",
            "Epoch [1/10], Step [16420/50782], Loss: 6.9535\n",
            "Epoch [1/10], Step [16430/50782], Loss: 7.1962\n",
            "Epoch [1/10], Step [16440/50782], Loss: 7.0716\n",
            "Epoch [1/10], Step [16450/50782], Loss: 6.8148\n",
            "Epoch [1/10], Step [16460/50782], Loss: 6.3375\n",
            "Epoch [1/10], Step [16470/50782], Loss: 8.6245\n",
            "Epoch [1/10], Step [16480/50782], Loss: 8.0000\n",
            "Epoch [1/10], Step [16490/50782], Loss: 6.8641\n",
            "Epoch [1/10], Step [16500/50782], Loss: 7.5641\n",
            "Epoch [1/10], Step [16510/50782], Loss: 6.0817\n",
            "Epoch [1/10], Step [16520/50782], Loss: 8.6964\n",
            "Epoch [1/10], Step [16530/50782], Loss: 6.9124\n",
            "Epoch [1/10], Step [16540/50782], Loss: 6.9413\n",
            "Epoch [1/10], Step [16550/50782], Loss: 6.9131\n",
            "Epoch [1/10], Step [16560/50782], Loss: 6.7374\n",
            "Epoch [1/10], Step [16570/50782], Loss: 8.4562\n",
            "Epoch [1/10], Step [16580/50782], Loss: 7.9978\n",
            "Epoch [1/10], Step [16590/50782], Loss: 6.7895\n",
            "Epoch [1/10], Step [16600/50782], Loss: 6.6845\n",
            "Epoch [1/10], Step [16610/50782], Loss: 7.0455\n",
            "Epoch [1/10], Step [16620/50782], Loss: 7.6288\n",
            "Epoch [1/10], Step [16630/50782], Loss: 6.7824\n",
            "Epoch [1/10], Step [16640/50782], Loss: 5.8260\n",
            "Epoch [1/10], Step [16650/50782], Loss: 8.1290\n",
            "Epoch [1/10], Step [16660/50782], Loss: 7.4366\n",
            "Epoch [1/10], Step [16670/50782], Loss: 8.6246\n",
            "Epoch [1/10], Step [16680/50782], Loss: 8.4499\n",
            "Epoch [1/10], Step [16690/50782], Loss: 8.2597\n",
            "Epoch [1/10], Step [16700/50782], Loss: 10.1358\n",
            "Epoch [1/10], Step [16710/50782], Loss: 7.5627\n",
            "Epoch [1/10], Step [16720/50782], Loss: 7.1663\n",
            "Epoch [1/10], Step [16730/50782], Loss: 6.8405\n",
            "Epoch [1/10], Step [16740/50782], Loss: 7.3702\n",
            "Epoch [1/10], Step [16750/50782], Loss: 7.5292\n",
            "Epoch [1/10], Step [16760/50782], Loss: 6.3078\n",
            "Epoch [1/10], Step [16770/50782], Loss: 6.6731\n",
            "Epoch [1/10], Step [16780/50782], Loss: 7.6420\n",
            "Epoch [1/10], Step [16790/50782], Loss: 7.8425\n",
            "Epoch [1/10], Step [16800/50782], Loss: 6.8136\n",
            "Epoch [1/10], Step [16810/50782], Loss: 9.9272\n",
            "Epoch [1/10], Step [16820/50782], Loss: 8.0620\n",
            "Epoch [1/10], Step [16830/50782], Loss: 5.8378\n",
            "Epoch [1/10], Step [16840/50782], Loss: 7.2930\n",
            "Epoch [1/10], Step [16850/50782], Loss: 7.1882\n",
            "Epoch [1/10], Step [16860/50782], Loss: 7.9915\n",
            "Epoch [1/10], Step [16870/50782], Loss: 7.9163\n",
            "Epoch [1/10], Step [16880/50782], Loss: 7.9379\n",
            "Epoch [1/10], Step [16890/50782], Loss: 7.5501\n",
            "Epoch [1/10], Step [16900/50782], Loss: 7.9123\n",
            "Epoch [1/10], Step [16910/50782], Loss: 7.4366\n",
            "Epoch [1/10], Step [16920/50782], Loss: 8.1772\n",
            "Epoch [1/10], Step [16930/50782], Loss: 5.9559\n",
            "Epoch [1/10], Step [16940/50782], Loss: 6.1824\n",
            "Epoch [1/10], Step [16950/50782], Loss: 8.6435\n",
            "Epoch [1/10], Step [16960/50782], Loss: 7.7908\n",
            "Epoch [1/10], Step [16970/50782], Loss: 9.3648\n",
            "Epoch [1/10], Step [16980/50782], Loss: 7.7360\n",
            "Epoch [1/10], Step [16990/50782], Loss: 8.1500\n",
            "Epoch [1/10], Step [17000/50782], Loss: 6.8905\n",
            "Epoch [1/10], Step [17010/50782], Loss: 7.5509\n",
            "Epoch [1/10], Step [17020/50782], Loss: 7.0201\n",
            "Epoch [1/10], Step [17030/50782], Loss: 8.6403\n",
            "Epoch [1/10], Step [17040/50782], Loss: 7.5612\n",
            "Epoch [1/10], Step [17050/50782], Loss: 7.5213\n",
            "Epoch [1/10], Step [17060/50782], Loss: 7.5847\n",
            "Epoch [1/10], Step [17070/50782], Loss: 6.5156\n",
            "Epoch [1/10], Step [17080/50782], Loss: 5.3687\n",
            "Epoch [1/10], Step [17090/50782], Loss: 5.5677\n",
            "Epoch [1/10], Step [17100/50782], Loss: 7.5484\n",
            "Epoch [1/10], Step [17110/50782], Loss: 8.4652\n",
            "Epoch [1/10], Step [17120/50782], Loss: 6.8112\n",
            "Epoch [1/10], Step [17130/50782], Loss: 7.0879\n",
            "Epoch [1/10], Step [17140/50782], Loss: 8.8942\n",
            "Epoch [1/10], Step [17150/50782], Loss: 6.8166\n",
            "Epoch [1/10], Step [17160/50782], Loss: 8.4355\n",
            "Epoch [1/10], Step [17170/50782], Loss: 7.2466\n",
            "Epoch [1/10], Step [17180/50782], Loss: 6.2218\n",
            "Epoch [1/10], Step [17190/50782], Loss: 7.5716\n",
            "Epoch [1/10], Step [17200/50782], Loss: 7.1822\n",
            "Epoch [1/10], Step [17210/50782], Loss: 6.3684\n",
            "Epoch [1/10], Step [17220/50782], Loss: 6.6307\n",
            "Epoch [1/10], Step [17230/50782], Loss: 6.3802\n",
            "Epoch [1/10], Step [17240/50782], Loss: 7.3804\n",
            "Epoch [1/10], Step [17250/50782], Loss: 8.7446\n",
            "Epoch [1/10], Step [17260/50782], Loss: 6.7151\n",
            "Epoch [1/10], Step [17270/50782], Loss: 6.8094\n",
            "Epoch [1/10], Step [17280/50782], Loss: 7.4428\n",
            "Epoch [1/10], Step [17290/50782], Loss: 7.6181\n",
            "Epoch [1/10], Step [17300/50782], Loss: 8.4313\n",
            "Epoch [1/10], Step [17310/50782], Loss: 7.4434\n",
            "Epoch [1/10], Step [17320/50782], Loss: 6.3243\n",
            "Epoch [1/10], Step [17330/50782], Loss: 7.4903\n",
            "Epoch [1/10], Step [17340/50782], Loss: 8.0850\n",
            "Epoch [1/10], Step [17350/50782], Loss: 7.0537\n",
            "Epoch [1/10], Step [17360/50782], Loss: 7.1130\n",
            "Epoch [1/10], Step [17370/50782], Loss: 6.3323\n",
            "Epoch [1/10], Step [17380/50782], Loss: 7.2422\n",
            "Epoch [1/10], Step [17390/50782], Loss: 6.6856\n",
            "Epoch [1/10], Step [17400/50782], Loss: 8.2398\n",
            "Epoch [1/10], Step [17410/50782], Loss: 7.4160\n",
            "Epoch [1/10], Step [17420/50782], Loss: 6.0079\n",
            "Epoch [1/10], Step [17430/50782], Loss: 7.6628\n",
            "Epoch [1/10], Step [17440/50782], Loss: 8.9746\n",
            "Epoch [1/10], Step [17450/50782], Loss: 7.2646\n",
            "Epoch [1/10], Step [17460/50782], Loss: 7.2884\n",
            "Epoch [1/10], Step [17470/50782], Loss: 7.2735\n",
            "Epoch [1/10], Step [17480/50782], Loss: 9.0840\n",
            "Epoch [1/10], Step [17490/50782], Loss: 6.9926\n",
            "Epoch [1/10], Step [17500/50782], Loss: 6.7944\n",
            "Epoch [1/10], Step [17510/50782], Loss: 6.0323\n",
            "Epoch [1/10], Step [17520/50782], Loss: 6.9591\n",
            "Epoch [1/10], Step [17530/50782], Loss: 7.8097\n",
            "Epoch [1/10], Step [17540/50782], Loss: 8.0265\n",
            "Epoch [1/10], Step [17550/50782], Loss: 6.3589\n",
            "Epoch [1/10], Step [17560/50782], Loss: 7.7084\n",
            "Epoch [1/10], Step [17570/50782], Loss: 6.7589\n",
            "Epoch [1/10], Step [17580/50782], Loss: 8.7443\n",
            "Epoch [1/10], Step [17590/50782], Loss: 6.3915\n",
            "Epoch [1/10], Step [17600/50782], Loss: 6.6616\n",
            "Epoch [1/10], Step [17610/50782], Loss: 7.8220\n",
            "Epoch [1/10], Step [17620/50782], Loss: 8.8065\n",
            "Epoch [1/10], Step [17630/50782], Loss: 7.8020\n",
            "Epoch [1/10], Step [17640/50782], Loss: 7.8400\n",
            "Epoch [1/10], Step [17650/50782], Loss: 8.0051\n",
            "Epoch [1/10], Step [17660/50782], Loss: 7.9343\n",
            "Epoch [1/10], Step [17670/50782], Loss: 8.8523\n",
            "Epoch [1/10], Step [17680/50782], Loss: 6.3693\n",
            "Epoch [1/10], Step [17690/50782], Loss: 8.8290\n",
            "Epoch [1/10], Step [17700/50782], Loss: 8.0262\n",
            "Epoch [1/10], Step [17710/50782], Loss: 7.6054\n",
            "Epoch [1/10], Step [17720/50782], Loss: 6.5176\n",
            "Epoch [1/10], Step [17730/50782], Loss: 7.1981\n",
            "Epoch [1/10], Step [17740/50782], Loss: 8.1814\n",
            "Epoch [1/10], Step [17750/50782], Loss: 6.4730\n",
            "Epoch [1/10], Step [17760/50782], Loss: 7.6191\n",
            "Epoch [1/10], Step [17770/50782], Loss: 6.9685\n",
            "Epoch [1/10], Step [17780/50782], Loss: 6.8165\n",
            "Epoch [1/10], Step [17790/50782], Loss: 6.8626\n",
            "Epoch [1/10], Step [17800/50782], Loss: 6.1275\n",
            "Epoch [1/10], Step [17810/50782], Loss: 8.5641\n",
            "Epoch [1/10], Step [17820/50782], Loss: 8.6514\n",
            "Epoch [1/10], Step [17830/50782], Loss: 7.0597\n",
            "Epoch [1/10], Step [17840/50782], Loss: 7.7172\n",
            "Epoch [1/10], Step [17850/50782], Loss: 7.6083\n",
            "Epoch [1/10], Step [17860/50782], Loss: 8.3218\n",
            "Epoch [1/10], Step [17870/50782], Loss: 7.1210\n",
            "Epoch [1/10], Step [17880/50782], Loss: 8.3962\n",
            "Epoch [1/10], Step [17890/50782], Loss: 7.2091\n",
            "Epoch [1/10], Step [17900/50782], Loss: 6.6666\n",
            "Epoch [1/10], Step [17910/50782], Loss: 7.2329\n",
            "Epoch [1/10], Step [17920/50782], Loss: 6.2601\n",
            "Epoch [1/10], Step [17930/50782], Loss: 7.6928\n",
            "Epoch [1/10], Step [17940/50782], Loss: 7.6191\n",
            "Epoch [1/10], Step [17950/50782], Loss: 7.7612\n",
            "Epoch [1/10], Step [17960/50782], Loss: 8.1734\n",
            "Epoch [1/10], Step [17970/50782], Loss: 7.8025\n",
            "Epoch [1/10], Step [17980/50782], Loss: 6.7393\n",
            "Epoch [1/10], Step [17990/50782], Loss: 6.8818\n",
            "Epoch [1/10], Step [18000/50782], Loss: 8.1290\n",
            "Epoch [1/10], Step [18010/50782], Loss: 7.7846\n",
            "Epoch [1/10], Step [18020/50782], Loss: 6.8466\n",
            "Epoch [1/10], Step [18030/50782], Loss: 9.4166\n",
            "Epoch [1/10], Step [18040/50782], Loss: 7.1796\n",
            "Epoch [1/10], Step [18050/50782], Loss: 6.8813\n",
            "Epoch [1/10], Step [18060/50782], Loss: 9.4942\n",
            "Epoch [1/10], Step [18070/50782], Loss: 8.3765\n",
            "Epoch [1/10], Step [18080/50782], Loss: 6.9546\n",
            "Epoch [1/10], Step [18090/50782], Loss: 6.3890\n",
            "Epoch [1/10], Step [18100/50782], Loss: 7.2674\n",
            "Epoch [1/10], Step [18110/50782], Loss: 7.6425\n",
            "Epoch [1/10], Step [18120/50782], Loss: 6.5103\n",
            "Epoch [1/10], Step [18130/50782], Loss: 7.4671\n",
            "Epoch [1/10], Step [18140/50782], Loss: 6.8006\n",
            "Epoch [1/10], Step [18150/50782], Loss: 5.6743\n",
            "Epoch [1/10], Step [18160/50782], Loss: 7.6929\n",
            "Epoch [1/10], Step [18170/50782], Loss: 6.6139\n",
            "Epoch [1/10], Step [18180/50782], Loss: 5.5742\n",
            "Epoch [1/10], Step [18190/50782], Loss: 8.0973\n",
            "Epoch [1/10], Step [18200/50782], Loss: 6.4168\n",
            "Epoch [1/10], Step [18210/50782], Loss: 7.7329\n",
            "Epoch [1/10], Step [18220/50782], Loss: 6.9060\n",
            "Epoch [1/10], Step [18230/50782], Loss: 6.7951\n",
            "Epoch [1/10], Step [18240/50782], Loss: 8.3436\n",
            "Epoch [1/10], Step [18250/50782], Loss: 7.1152\n",
            "Epoch [1/10], Step [18260/50782], Loss: 7.0771\n",
            "Epoch [1/10], Step [18270/50782], Loss: 6.5426\n",
            "Epoch [1/10], Step [18280/50782], Loss: 7.6509\n",
            "Epoch [1/10], Step [18290/50782], Loss: 7.1250\n",
            "Epoch [1/10], Step [18300/50782], Loss: 7.1572\n",
            "Epoch [1/10], Step [18310/50782], Loss: 7.7242\n",
            "Epoch [1/10], Step [18320/50782], Loss: 7.2722\n",
            "Epoch [1/10], Step [18330/50782], Loss: 6.8097\n",
            "Epoch [1/10], Step [18340/50782], Loss: 7.0056\n",
            "Epoch [1/10], Step [18350/50782], Loss: 7.3083\n",
            "Epoch [1/10], Step [18360/50782], Loss: 8.1710\n",
            "Epoch [1/10], Step [18370/50782], Loss: 9.0563\n",
            "Epoch [1/10], Step [18380/50782], Loss: 6.7045\n",
            "Epoch [1/10], Step [18390/50782], Loss: 6.6968\n",
            "Epoch [1/10], Step [18400/50782], Loss: 8.9160\n",
            "Epoch [1/10], Step [18410/50782], Loss: 6.7408\n",
            "Epoch [1/10], Step [18420/50782], Loss: 7.4709\n",
            "Epoch [1/10], Step [18430/50782], Loss: 7.3774\n",
            "Epoch [1/10], Step [18440/50782], Loss: 7.4692\n",
            "Epoch [1/10], Step [18450/50782], Loss: 8.2175\n",
            "Epoch [1/10], Step [18460/50782], Loss: 8.6671\n",
            "Epoch [1/10], Step [18470/50782], Loss: 8.5404\n",
            "Epoch [1/10], Step [18480/50782], Loss: 7.5495\n",
            "Epoch [1/10], Step [18490/50782], Loss: 7.2517\n",
            "Epoch [1/10], Step [18500/50782], Loss: 6.9456\n",
            "Epoch [1/10], Step [18510/50782], Loss: 6.7480\n",
            "Epoch [1/10], Step [18520/50782], Loss: 8.1307\n",
            "Epoch [1/10], Step [18530/50782], Loss: 6.4286\n",
            "Epoch [1/10], Step [18540/50782], Loss: 6.7004\n",
            "Epoch [1/10], Step [18550/50782], Loss: 7.7298\n",
            "Epoch [1/10], Step [18560/50782], Loss: 7.2281\n",
            "Epoch [1/10], Step [18570/50782], Loss: 7.0938\n",
            "Epoch [1/10], Step [18580/50782], Loss: 6.5758\n",
            "Epoch [1/10], Step [18590/50782], Loss: 7.4841\n",
            "Epoch [1/10], Step [18600/50782], Loss: 7.4959\n",
            "Epoch [1/10], Step [18610/50782], Loss: 7.6759\n",
            "Epoch [1/10], Step [18620/50782], Loss: 6.2334\n",
            "Epoch [1/10], Step [18630/50782], Loss: 8.6663\n",
            "Epoch [1/10], Step [18640/50782], Loss: 7.0989\n",
            "Epoch [1/10], Step [18650/50782], Loss: 7.5234\n",
            "Epoch [1/10], Step [18660/50782], Loss: 7.2712\n",
            "Epoch [1/10], Step [18670/50782], Loss: 6.2689\n",
            "Epoch [1/10], Step [18680/50782], Loss: 8.7617\n",
            "Epoch [1/10], Step [18690/50782], Loss: 6.5213\n",
            "Epoch [1/10], Step [18700/50782], Loss: 6.7664\n",
            "Epoch [1/10], Step [18710/50782], Loss: 8.1327\n",
            "Epoch [1/10], Step [18720/50782], Loss: 7.9741\n",
            "Epoch [1/10], Step [18730/50782], Loss: 6.8747\n",
            "Epoch [1/10], Step [18740/50782], Loss: 7.1837\n",
            "Epoch [1/10], Step [18750/50782], Loss: 7.0371\n",
            "Epoch [1/10], Step [18760/50782], Loss: 8.6317\n",
            "Epoch [1/10], Step [18770/50782], Loss: 6.0552\n",
            "Epoch [1/10], Step [18780/50782], Loss: 8.3500\n",
            "Epoch [1/10], Step [18790/50782], Loss: 7.7857\n",
            "Epoch [1/10], Step [18800/50782], Loss: 8.5115\n",
            "Epoch [1/10], Step [18810/50782], Loss: 5.9068\n",
            "Epoch [1/10], Step [18820/50782], Loss: 7.9792\n",
            "Epoch [1/10], Step [18830/50782], Loss: 7.2703\n",
            "Epoch [1/10], Step [18840/50782], Loss: 8.8584\n",
            "Epoch [1/10], Step [18850/50782], Loss: 9.4918\n",
            "Epoch [1/10], Step [18860/50782], Loss: 6.8626\n",
            "Epoch [1/10], Step [18870/50782], Loss: 5.1607\n",
            "Epoch [1/10], Step [18880/50782], Loss: 8.0405\n",
            "Epoch [1/10], Step [18890/50782], Loss: 5.8288\n",
            "Epoch [1/10], Step [18900/50782], Loss: 7.2294\n",
            "Epoch [1/10], Step [18910/50782], Loss: 5.7120\n",
            "Epoch [1/10], Step [18920/50782], Loss: 8.0137\n",
            "Epoch [1/10], Step [18930/50782], Loss: 7.2166\n",
            "Epoch [1/10], Step [18940/50782], Loss: 7.8987\n",
            "Epoch [1/10], Step [18950/50782], Loss: 8.6602\n",
            "Epoch [1/10], Step [18960/50782], Loss: 7.5631\n",
            "Epoch [1/10], Step [18970/50782], Loss: 5.9688\n",
            "Epoch [1/10], Step [18980/50782], Loss: 7.3085\n",
            "Epoch [1/10], Step [18990/50782], Loss: 7.5431\n",
            "Epoch [1/10], Step [19000/50782], Loss: 7.3189\n",
            "Epoch [1/10], Step [19010/50782], Loss: 6.4521\n",
            "Epoch [1/10], Step [19020/50782], Loss: 5.3853\n",
            "Epoch [1/10], Step [19030/50782], Loss: 6.9758\n",
            "Epoch [1/10], Step [19040/50782], Loss: 7.6096\n",
            "Epoch [1/10], Step [19050/50782], Loss: 7.2932\n",
            "Epoch [1/10], Step [19060/50782], Loss: 8.8327\n",
            "Epoch [1/10], Step [19070/50782], Loss: 6.2996\n",
            "Epoch [1/10], Step [19080/50782], Loss: 7.6670\n",
            "Epoch [1/10], Step [19090/50782], Loss: 7.7036\n",
            "Epoch [1/10], Step [19100/50782], Loss: 7.5611\n",
            "Epoch [1/10], Step [19110/50782], Loss: 7.5389\n",
            "Epoch [1/10], Step [19120/50782], Loss: 7.0163\n",
            "Epoch [1/10], Step [19130/50782], Loss: 6.9602\n",
            "Epoch [1/10], Step [19140/50782], Loss: 7.2548\n",
            "Epoch [1/10], Step [19150/50782], Loss: 8.1145\n",
            "Epoch [1/10], Step [19160/50782], Loss: 7.2011\n",
            "Epoch [1/10], Step [19170/50782], Loss: 6.7047\n",
            "Epoch [1/10], Step [19180/50782], Loss: 7.2774\n",
            "Epoch [1/10], Step [19190/50782], Loss: 8.0453\n",
            "Epoch [1/10], Step [19200/50782], Loss: 10.1830\n",
            "Epoch [1/10], Step [19210/50782], Loss: 7.5001\n",
            "Epoch [1/10], Step [19220/50782], Loss: 6.8722\n",
            "Epoch [1/10], Step [19230/50782], Loss: 8.6414\n",
            "Epoch [1/10], Step [19240/50782], Loss: 8.0738\n",
            "Epoch [1/10], Step [19250/50782], Loss: 7.4029\n",
            "Epoch [1/10], Step [19260/50782], Loss: 7.4803\n",
            "Epoch [1/10], Step [19270/50782], Loss: 6.2348\n",
            "Epoch [1/10], Step [19280/50782], Loss: 7.7223\n",
            "Epoch [1/10], Step [19290/50782], Loss: 7.4810\n",
            "Epoch [1/10], Step [19300/50782], Loss: 7.1505\n",
            "Epoch [1/10], Step [19310/50782], Loss: 7.6768\n",
            "Epoch [1/10], Step [19320/50782], Loss: 6.7843\n",
            "Epoch [1/10], Step [19330/50782], Loss: 7.1585\n",
            "Epoch [1/10], Step [19340/50782], Loss: 8.5500\n",
            "Epoch [1/10], Step [19350/50782], Loss: 8.2665\n",
            "Epoch [1/10], Step [19360/50782], Loss: 7.3487\n",
            "Epoch [1/10], Step [19370/50782], Loss: 6.9494\n",
            "Epoch [1/10], Step [19380/50782], Loss: 8.0863\n",
            "Epoch [1/10], Step [19390/50782], Loss: 5.6694\n",
            "Epoch [1/10], Step [19400/50782], Loss: 7.7702\n",
            "Epoch [1/10], Step [19410/50782], Loss: 7.3048\n",
            "Epoch [1/10], Step [19420/50782], Loss: 6.6600\n",
            "Epoch [1/10], Step [19430/50782], Loss: 7.8711\n",
            "Epoch [1/10], Step [19440/50782], Loss: 7.2350\n",
            "Epoch [1/10], Step [19450/50782], Loss: 8.7177\n",
            "Epoch [1/10], Step [19460/50782], Loss: 7.2700\n",
            "Epoch [1/10], Step [19470/50782], Loss: 6.1715\n",
            "Epoch [1/10], Step [19480/50782], Loss: 7.6707\n",
            "Epoch [1/10], Step [19490/50782], Loss: 8.3371\n",
            "Epoch [1/10], Step [19500/50782], Loss: 7.5978\n",
            "Epoch [1/10], Step [19510/50782], Loss: 8.6515\n",
            "Epoch [1/10], Step [19520/50782], Loss: 8.0448\n",
            "Epoch [1/10], Step [19530/50782], Loss: 6.9673\n",
            "Epoch [1/10], Step [19540/50782], Loss: 7.4575\n",
            "Epoch [1/10], Step [19550/50782], Loss: 6.0319\n",
            "Epoch [1/10], Step [19560/50782], Loss: 8.3762\n",
            "Epoch [1/10], Step [19570/50782], Loss: 8.6208\n",
            "Epoch [1/10], Step [19580/50782], Loss: 7.5559\n",
            "Epoch [1/10], Step [19590/50782], Loss: 6.4516\n",
            "Epoch [1/10], Step [19600/50782], Loss: 7.5504\n",
            "Epoch [1/10], Step [19610/50782], Loss: 7.3632\n",
            "Epoch [1/10], Step [19620/50782], Loss: 6.9690\n",
            "Epoch [1/10], Step [19630/50782], Loss: 7.1535\n",
            "Epoch [1/10], Step [19640/50782], Loss: 5.8278\n",
            "Epoch [1/10], Step [19650/50782], Loss: 6.5835\n",
            "Epoch [1/10], Step [19660/50782], Loss: 7.9937\n",
            "Epoch [1/10], Step [19670/50782], Loss: 6.1253\n",
            "Epoch [1/10], Step [19680/50782], Loss: 7.8579\n",
            "Epoch [1/10], Step [19690/50782], Loss: 9.8291\n",
            "Epoch [1/10], Step [19700/50782], Loss: 8.4886\n",
            "Epoch [1/10], Step [19710/50782], Loss: 6.3361\n",
            "Epoch [1/10], Step [19720/50782], Loss: 7.2054\n",
            "Epoch [1/10], Step [19730/50782], Loss: 8.9160\n",
            "Epoch [1/10], Step [19740/50782], Loss: 8.0462\n",
            "Epoch [1/10], Step [19750/50782], Loss: 6.3443\n",
            "Epoch [1/10], Step [19760/50782], Loss: 5.9608\n",
            "Epoch [1/10], Step [19770/50782], Loss: 6.8143\n",
            "Epoch [1/10], Step [19780/50782], Loss: 7.1801\n",
            "Epoch [1/10], Step [19790/50782], Loss: 7.2555\n",
            "Epoch [1/10], Step [19800/50782], Loss: 7.6551\n",
            "Epoch [1/10], Step [19810/50782], Loss: 7.5661\n",
            "Epoch [1/10], Step [19820/50782], Loss: 7.5543\n",
            "Epoch [1/10], Step [19830/50782], Loss: 6.1426\n",
            "Epoch [1/10], Step [19840/50782], Loss: 9.3181\n",
            "Epoch [1/10], Step [19850/50782], Loss: 7.5117\n",
            "Epoch [1/10], Step [19860/50782], Loss: 8.4282\n",
            "Epoch [1/10], Step [19870/50782], Loss: 5.9910\n",
            "Epoch [1/10], Step [19880/50782], Loss: 7.1309\n",
            "Epoch [1/10], Step [19890/50782], Loss: 7.1417\n",
            "Epoch [1/10], Step [19900/50782], Loss: 9.0495\n",
            "Epoch [1/10], Step [19910/50782], Loss: 6.0995\n",
            "Epoch [1/10], Step [19920/50782], Loss: 5.2234\n",
            "Epoch [1/10], Step [19930/50782], Loss: 9.0083\n",
            "Epoch [1/10], Step [19940/50782], Loss: 6.7665\n",
            "Epoch [1/10], Step [19950/50782], Loss: 8.4569\n",
            "Epoch [1/10], Step [19960/50782], Loss: 6.5555\n",
            "Epoch [1/10], Step [19970/50782], Loss: 9.8080\n",
            "Epoch [1/10], Step [19980/50782], Loss: 7.5961\n",
            "Epoch [1/10], Step [19990/50782], Loss: 6.0617\n",
            "Epoch [1/10], Step [20000/50782], Loss: 6.0178\n",
            "Epoch [1/10], Step [20010/50782], Loss: 7.9359\n",
            "Epoch [1/10], Step [20020/50782], Loss: 8.0437\n",
            "Epoch [1/10], Step [20030/50782], Loss: 7.2174\n",
            "Epoch [1/10], Step [20040/50782], Loss: 6.3617\n",
            "Epoch [1/10], Step [20050/50782], Loss: 7.2826\n",
            "Epoch [1/10], Step [20060/50782], Loss: 6.8018\n",
            "Epoch [1/10], Step [20070/50782], Loss: 8.4632\n",
            "Epoch [1/10], Step [20080/50782], Loss: 6.7471\n",
            "Epoch [1/10], Step [20090/50782], Loss: 8.2520\n",
            "Epoch [1/10], Step [20100/50782], Loss: 8.0648\n",
            "Epoch [1/10], Step [20110/50782], Loss: 7.7404\n",
            "Epoch [1/10], Step [20120/50782], Loss: 9.6178\n",
            "Epoch [1/10], Step [20130/50782], Loss: 6.9556\n",
            "Epoch [1/10], Step [20140/50782], Loss: 7.2265\n",
            "Epoch [1/10], Step [20150/50782], Loss: 8.7284\n",
            "Epoch [1/10], Step [20160/50782], Loss: 6.2232\n",
            "Epoch [1/10], Step [20170/50782], Loss: 6.7173\n",
            "Epoch [1/10], Step [20180/50782], Loss: 9.0076\n",
            "Epoch [1/10], Step [20190/50782], Loss: 7.9074\n",
            "Epoch [1/10], Step [20200/50782], Loss: 7.8956\n",
            "Epoch [1/10], Step [20210/50782], Loss: 6.9880\n",
            "Epoch [1/10], Step [20220/50782], Loss: 8.0810\n",
            "Epoch [1/10], Step [20230/50782], Loss: 8.8391\n",
            "Epoch [1/10], Step [20240/50782], Loss: 7.2252\n",
            "Epoch [1/10], Step [20250/50782], Loss: 7.6915\n",
            "Epoch [1/10], Step [20260/50782], Loss: 5.6219\n",
            "Epoch [1/10], Step [20270/50782], Loss: 6.0142\n",
            "Epoch [1/10], Step [20280/50782], Loss: 9.6824\n",
            "Epoch [1/10], Step [20290/50782], Loss: 7.7499\n",
            "Epoch [1/10], Step [20300/50782], Loss: 7.1616\n",
            "Epoch [1/10], Step [20310/50782], Loss: 6.7227\n",
            "Epoch [1/10], Step [20320/50782], Loss: 7.7351\n",
            "Epoch [1/10], Step [20330/50782], Loss: 7.0855\n",
            "Epoch [1/10], Step [20340/50782], Loss: 8.8353\n",
            "Epoch [1/10], Step [20350/50782], Loss: 8.8088\n",
            "Epoch [1/10], Step [20360/50782], Loss: 7.7702\n",
            "Epoch [1/10], Step [20370/50782], Loss: 7.1454\n",
            "Epoch [1/10], Step [20380/50782], Loss: 8.6618\n",
            "Epoch [1/10], Step [20390/50782], Loss: 6.8766\n",
            "Epoch [1/10], Step [20400/50782], Loss: 7.8902\n",
            "Epoch [1/10], Step [20410/50782], Loss: 6.6217\n",
            "Epoch [1/10], Step [20420/50782], Loss: 6.8932\n",
            "Epoch [1/10], Step [20430/50782], Loss: 6.3463\n",
            "Epoch [1/10], Step [20440/50782], Loss: 8.4372\n",
            "Epoch [1/10], Step [20450/50782], Loss: 8.6727\n",
            "Epoch [1/10], Step [20460/50782], Loss: 9.0740\n",
            "Epoch [1/10], Step [20470/50782], Loss: 7.1418\n",
            "Epoch [1/10], Step [20480/50782], Loss: 8.8805\n",
            "Epoch [1/10], Step [20490/50782], Loss: 5.5336\n",
            "Epoch [1/10], Step [20500/50782], Loss: 7.7124\n",
            "Epoch [1/10], Step [20510/50782], Loss: 7.9678\n",
            "Epoch [1/10], Step [20520/50782], Loss: 9.0670\n",
            "Epoch [1/10], Step [20530/50782], Loss: 6.7619\n",
            "Epoch [1/10], Step [20540/50782], Loss: 7.0256\n",
            "Epoch [1/10], Step [20550/50782], Loss: 8.6320\n",
            "Epoch [1/10], Step [20560/50782], Loss: 7.6619\n",
            "Epoch [1/10], Step [20570/50782], Loss: 8.3345\n",
            "Epoch [1/10], Step [20580/50782], Loss: 8.5071\n",
            "Epoch [1/10], Step [20590/50782], Loss: 8.4892\n",
            "Epoch [1/10], Step [20600/50782], Loss: 7.7707\n",
            "Epoch [1/10], Step [20610/50782], Loss: 8.2275\n",
            "Epoch [1/10], Step [20620/50782], Loss: 8.0662\n",
            "Epoch [1/10], Step [20630/50782], Loss: 6.0840\n",
            "Epoch [1/10], Step [20640/50782], Loss: 8.9246\n",
            "Epoch [1/10], Step [20650/50782], Loss: 6.5988\n",
            "Epoch [1/10], Step [20660/50782], Loss: 7.7721\n",
            "Epoch [1/10], Step [20670/50782], Loss: 8.2667\n",
            "Epoch [1/10], Step [20680/50782], Loss: 6.6094\n",
            "Epoch [1/10], Step [20690/50782], Loss: 6.9890\n",
            "Epoch [1/10], Step [20700/50782], Loss: 7.8451\n",
            "Epoch [1/10], Step [20710/50782], Loss: 7.5741\n",
            "Epoch [1/10], Step [20720/50782], Loss: 8.7322\n",
            "Epoch [1/10], Step [20730/50782], Loss: 7.9259\n",
            "Epoch [1/10], Step [20740/50782], Loss: 6.3921\n",
            "Epoch [1/10], Step [20750/50782], Loss: 7.1138\n",
            "Epoch [1/10], Step [20760/50782], Loss: 7.6715\n",
            "Epoch [1/10], Step [20770/50782], Loss: 8.1958\n",
            "Epoch [1/10], Step [20780/50782], Loss: 7.3654\n",
            "Epoch [1/10], Step [20790/50782], Loss: 8.3484\n",
            "Epoch [1/10], Step [20800/50782], Loss: 6.2964\n",
            "Epoch [1/10], Step [20810/50782], Loss: 5.3926\n",
            "Epoch [1/10], Step [20820/50782], Loss: 7.7741\n",
            "Epoch [1/10], Step [20830/50782], Loss: 6.9231\n",
            "Epoch [1/10], Step [20840/50782], Loss: 7.7266\n",
            "Epoch [1/10], Step [20850/50782], Loss: 6.9329\n",
            "Epoch [1/10], Step [20860/50782], Loss: 6.7497\n",
            "Epoch [1/10], Step [20870/50782], Loss: 7.5208\n",
            "Epoch [1/10], Step [20880/50782], Loss: 7.6668\n",
            "Epoch [1/10], Step [20890/50782], Loss: 7.0576\n",
            "Epoch [1/10], Step [20900/50782], Loss: 7.5035\n",
            "Epoch [1/10], Step [20910/50782], Loss: 6.8461\n",
            "Epoch [1/10], Step [20920/50782], Loss: 7.6310\n",
            "Epoch [1/10], Step [20930/50782], Loss: 6.1823\n",
            "Epoch [1/10], Step [20940/50782], Loss: 7.4705\n",
            "Epoch [1/10], Step [20950/50782], Loss: 9.4876\n",
            "Epoch [1/10], Step [20960/50782], Loss: 7.4998\n",
            "Epoch [1/10], Step [20970/50782], Loss: 7.7149\n",
            "Epoch [1/10], Step [20980/50782], Loss: 8.6847\n",
            "Epoch [1/10], Step [20990/50782], Loss: 7.3565\n",
            "Epoch [1/10], Step [21000/50782], Loss: 7.2493\n",
            "Epoch [1/10], Step [21010/50782], Loss: 7.7110\n",
            "Epoch [1/10], Step [21020/50782], Loss: 6.9749\n",
            "Epoch [1/10], Step [21030/50782], Loss: 6.1015\n",
            "Epoch [1/10], Step [21040/50782], Loss: 6.8449\n",
            "Epoch [1/10], Step [21050/50782], Loss: 9.4752\n",
            "Epoch [1/10], Step [21060/50782], Loss: 8.2863\n",
            "Epoch [1/10], Step [21070/50782], Loss: 8.6250\n",
            "Epoch [1/10], Step [21080/50782], Loss: 6.8569\n",
            "Epoch [1/10], Step [21090/50782], Loss: 7.6445\n",
            "Epoch [1/10], Step [21100/50782], Loss: 6.5667\n",
            "Epoch [1/10], Step [21110/50782], Loss: 7.7912\n",
            "Epoch [1/10], Step [21120/50782], Loss: 8.5248\n",
            "Epoch [1/10], Step [21130/50782], Loss: 7.4868\n",
            "Epoch [1/10], Step [21140/50782], Loss: 7.2569\n",
            "Epoch [1/10], Step [21150/50782], Loss: 7.2306\n",
            "Epoch [1/10], Step [21160/50782], Loss: 6.8320\n",
            "Epoch [1/10], Step [21170/50782], Loss: 6.9843\n",
            "Epoch [1/10], Step [21180/50782], Loss: 7.4120\n",
            "Epoch [1/10], Step [21190/50782], Loss: 8.6961\n",
            "Epoch [1/10], Step [21200/50782], Loss: 8.4164\n",
            "Epoch [1/10], Step [21210/50782], Loss: 8.7461\n",
            "Epoch [1/10], Step [21220/50782], Loss: 7.4803\n",
            "Epoch [1/10], Step [21230/50782], Loss: 7.6055\n",
            "Epoch [1/10], Step [21240/50782], Loss: 8.7397\n",
            "Epoch [1/10], Step [21250/50782], Loss: 7.3670\n",
            "Epoch [1/10], Step [21260/50782], Loss: 6.5261\n",
            "Epoch [1/10], Step [21270/50782], Loss: 7.1834\n",
            "Epoch [1/10], Step [21280/50782], Loss: 6.8995\n",
            "Epoch [1/10], Step [21290/50782], Loss: 6.9584\n",
            "Epoch [1/10], Step [21300/50782], Loss: 8.1222\n",
            "Epoch [1/10], Step [21310/50782], Loss: 7.5787\n",
            "Epoch [1/10], Step [21320/50782], Loss: 7.7934\n",
            "Epoch [1/10], Step [21330/50782], Loss: 7.5430\n",
            "Epoch [1/10], Step [21340/50782], Loss: 7.9626\n",
            "Epoch [1/10], Step [21350/50782], Loss: 7.2188\n",
            "Epoch [1/10], Step [21360/50782], Loss: 7.1822\n",
            "Epoch [1/10], Step [21370/50782], Loss: 7.9691\n",
            "Epoch [1/10], Step [21380/50782], Loss: 6.8632\n",
            "Epoch [1/10], Step [21390/50782], Loss: 7.5172\n",
            "Epoch [1/10], Step [21400/50782], Loss: 9.8421\n",
            "Epoch [1/10], Step [21410/50782], Loss: 9.8740\n",
            "Epoch [1/10], Step [21420/50782], Loss: 8.2717\n",
            "Epoch [1/10], Step [21430/50782], Loss: 7.0940\n",
            "Epoch [1/10], Step [21440/50782], Loss: 6.4423\n",
            "Epoch [1/10], Step [21450/50782], Loss: 6.3672\n",
            "Epoch [1/10], Step [21460/50782], Loss: 8.7728\n",
            "Epoch [1/10], Step [21470/50782], Loss: 7.3827\n",
            "Epoch [1/10], Step [21480/50782], Loss: 6.5093\n",
            "Epoch [1/10], Step [21490/50782], Loss: 6.1098\n",
            "Epoch [1/10], Step [21500/50782], Loss: 7.3832\n",
            "Epoch [1/10], Step [21510/50782], Loss: 5.7544\n",
            "Epoch [1/10], Step [21520/50782], Loss: 7.1133\n",
            "Epoch [1/10], Step [21530/50782], Loss: 7.6873\n",
            "Epoch [1/10], Step [21540/50782], Loss: 7.9058\n",
            "Epoch [1/10], Step [21550/50782], Loss: 6.7208\n",
            "Epoch [1/10], Step [21560/50782], Loss: 6.8915\n",
            "Epoch [1/10], Step [21570/50782], Loss: 6.3751\n",
            "Epoch [1/10], Step [21580/50782], Loss: 7.3308\n",
            "Epoch [1/10], Step [21590/50782], Loss: 8.1841\n",
            "Epoch [1/10], Step [21600/50782], Loss: 9.8756\n",
            "Epoch [1/10], Step [21610/50782], Loss: 7.2078\n",
            "Epoch [1/10], Step [21620/50782], Loss: 8.1213\n",
            "Epoch [1/10], Step [21630/50782], Loss: 8.0302\n",
            "Epoch [1/10], Step [21640/50782], Loss: 5.7993\n",
            "Epoch [1/10], Step [21650/50782], Loss: 7.8496\n",
            "Epoch [1/10], Step [21660/50782], Loss: 8.5857\n",
            "Epoch [1/10], Step [21670/50782], Loss: 8.3429\n",
            "Epoch [1/10], Step [21680/50782], Loss: 8.7566\n",
            "Epoch [1/10], Step [21690/50782], Loss: 7.0015\n",
            "Epoch [1/10], Step [21700/50782], Loss: 7.4035\n",
            "Epoch [1/10], Step [21710/50782], Loss: 6.9912\n",
            "Epoch [1/10], Step [21720/50782], Loss: 7.2233\n",
            "Epoch [1/10], Step [21730/50782], Loss: 7.1764\n",
            "Epoch [1/10], Step [21740/50782], Loss: 8.6275\n",
            "Epoch [1/10], Step [21750/50782], Loss: 8.2240\n",
            "Epoch [1/10], Step [21760/50782], Loss: 8.0410\n",
            "Epoch [1/10], Step [21770/50782], Loss: 6.6292\n",
            "Epoch [1/10], Step [21780/50782], Loss: 5.2550\n",
            "Epoch [1/10], Step [21790/50782], Loss: 6.3183\n",
            "Epoch [1/10], Step [21800/50782], Loss: 7.0815\n",
            "Epoch [1/10], Step [21810/50782], Loss: 7.3107\n",
            "Epoch [1/10], Step [21820/50782], Loss: 9.1218\n",
            "Epoch [1/10], Step [21830/50782], Loss: 6.5309\n",
            "Epoch [1/10], Step [21840/50782], Loss: 6.7361\n",
            "Epoch [1/10], Step [21850/50782], Loss: 7.3978\n",
            "Epoch [1/10], Step [21860/50782], Loss: 6.3831\n",
            "Epoch [1/10], Step [21870/50782], Loss: 6.7912\n",
            "Epoch [1/10], Step [21880/50782], Loss: 6.2451\n",
            "Epoch [1/10], Step [21890/50782], Loss: 6.9923\n",
            "Epoch [1/10], Step [21900/50782], Loss: 7.9609\n",
            "Epoch [1/10], Step [21910/50782], Loss: 7.1974\n",
            "Epoch [1/10], Step [21920/50782], Loss: 8.0720\n",
            "Epoch [1/10], Step [21930/50782], Loss: 6.3838\n",
            "Epoch [1/10], Step [21940/50782], Loss: 8.2239\n",
            "Epoch [1/10], Step [21950/50782], Loss: 6.4376\n",
            "Epoch [1/10], Step [21960/50782], Loss: 7.1508\n",
            "Epoch [1/10], Step [21970/50782], Loss: 9.2021\n",
            "Epoch [1/10], Step [21980/50782], Loss: 6.0029\n",
            "Epoch [1/10], Step [21990/50782], Loss: 7.3412\n",
            "Epoch [1/10], Step [22000/50782], Loss: 6.9161\n",
            "Epoch [1/10], Step [22010/50782], Loss: 7.8582\n",
            "Epoch [1/10], Step [22020/50782], Loss: 9.2365\n",
            "Epoch [1/10], Step [22030/50782], Loss: 7.7272\n",
            "Epoch [1/10], Step [22040/50782], Loss: 8.7457\n",
            "Epoch [1/10], Step [22050/50782], Loss: 8.2908\n",
            "Epoch [1/10], Step [22060/50782], Loss: 8.2000\n",
            "Epoch [1/10], Step [22070/50782], Loss: 7.9808\n",
            "Epoch [1/10], Step [22080/50782], Loss: 8.3664\n",
            "Epoch [1/10], Step [22090/50782], Loss: 7.5631\n",
            "Epoch [1/10], Step [22100/50782], Loss: 6.4766\n",
            "Epoch [1/10], Step [22110/50782], Loss: 8.2685\n",
            "Epoch [1/10], Step [22120/50782], Loss: 7.1735\n",
            "Epoch [1/10], Step [22130/50782], Loss: 6.7334\n",
            "Epoch [1/10], Step [22140/50782], Loss: 8.5185\n",
            "Epoch [1/10], Step [22150/50782], Loss: 7.0727\n",
            "Epoch [1/10], Step [22160/50782], Loss: 6.4959\n",
            "Epoch [1/10], Step [22170/50782], Loss: 9.2836\n",
            "Epoch [1/10], Step [22180/50782], Loss: 7.3554\n",
            "Epoch [1/10], Step [22190/50782], Loss: 7.0930\n",
            "Epoch [1/10], Step [22200/50782], Loss: 7.6787\n",
            "Epoch [1/10], Step [22210/50782], Loss: 7.0683\n",
            "Epoch [1/10], Step [22220/50782], Loss: 7.8601\n",
            "Epoch [1/10], Step [22230/50782], Loss: 7.2954\n",
            "Epoch [1/10], Step [22240/50782], Loss: 5.8232\n",
            "Epoch [1/10], Step [22250/50782], Loss: 9.0379\n",
            "Epoch [1/10], Step [22260/50782], Loss: 6.6270\n",
            "Epoch [1/10], Step [22270/50782], Loss: 8.3168\n",
            "Epoch [1/10], Step [22280/50782], Loss: 9.2596\n",
            "Epoch [1/10], Step [22290/50782], Loss: 6.4725\n",
            "Epoch [1/10], Step [22300/50782], Loss: 8.5848\n",
            "Epoch [1/10], Step [22310/50782], Loss: 6.7235\n",
            "Epoch [1/10], Step [22320/50782], Loss: 7.9638\n",
            "Epoch [1/10], Step [22330/50782], Loss: 6.9350\n",
            "Epoch [1/10], Step [22340/50782], Loss: 6.2144\n",
            "Epoch [1/10], Step [22350/50782], Loss: 6.9222\n",
            "Epoch [1/10], Step [22360/50782], Loss: 6.8484\n",
            "Epoch [1/10], Step [22370/50782], Loss: 7.7236\n",
            "Epoch [1/10], Step [22380/50782], Loss: 6.6753\n",
            "Epoch [1/10], Step [22390/50782], Loss: 6.9370\n",
            "Epoch [1/10], Step [22400/50782], Loss: 6.8764\n",
            "Epoch [1/10], Step [22410/50782], Loss: 7.9230\n",
            "Epoch [1/10], Step [22420/50782], Loss: 7.9335\n",
            "Epoch [1/10], Step [22430/50782], Loss: 6.2714\n",
            "Epoch [1/10], Step [22440/50782], Loss: 9.4885\n",
            "Epoch [1/10], Step [22450/50782], Loss: 7.9316\n",
            "Epoch [1/10], Step [22460/50782], Loss: 6.2532\n",
            "Epoch [1/10], Step [22470/50782], Loss: 8.3318\n",
            "Epoch [1/10], Step [22480/50782], Loss: 6.9110\n",
            "Epoch [1/10], Step [22490/50782], Loss: 7.9370\n",
            "Epoch [1/10], Step [22500/50782], Loss: 6.7837\n",
            "Epoch [1/10], Step [22510/50782], Loss: 7.5137\n",
            "Epoch [1/10], Step [22520/50782], Loss: 8.6750\n",
            "Epoch [1/10], Step [22530/50782], Loss: 7.8421\n",
            "Epoch [1/10], Step [22540/50782], Loss: 7.7861\n",
            "Epoch [1/10], Step [22550/50782], Loss: 7.6213\n",
            "Epoch [1/10], Step [22560/50782], Loss: 6.7995\n",
            "Epoch [1/10], Step [22570/50782], Loss: 8.1892\n",
            "Epoch [1/10], Step [22580/50782], Loss: 6.2804\n",
            "Epoch [1/10], Step [22590/50782], Loss: 8.4228\n",
            "Epoch [1/10], Step [22600/50782], Loss: 6.2803\n",
            "Epoch [1/10], Step [22610/50782], Loss: 5.9018\n",
            "Epoch [1/10], Step [22620/50782], Loss: 7.2374\n",
            "Epoch [1/10], Step [22630/50782], Loss: 9.2090\n",
            "Epoch [1/10], Step [22640/50782], Loss: 7.5729\n",
            "Epoch [1/10], Step [22650/50782], Loss: 8.0051\n",
            "Epoch [1/10], Step [22660/50782], Loss: 7.1117\n",
            "Epoch [1/10], Step [22670/50782], Loss: 6.5544\n",
            "Epoch [1/10], Step [22680/50782], Loss: 5.8313\n",
            "Epoch [1/10], Step [22690/50782], Loss: 6.4538\n",
            "Epoch [1/10], Step [22700/50782], Loss: 8.3374\n",
            "Epoch [1/10], Step [22710/50782], Loss: 6.9615\n",
            "Epoch [1/10], Step [22720/50782], Loss: 7.4175\n",
            "Epoch [1/10], Step [22730/50782], Loss: 6.9494\n",
            "Epoch [1/10], Step [22740/50782], Loss: 8.4432\n",
            "Epoch [1/10], Step [22750/50782], Loss: 6.6131\n",
            "Epoch [1/10], Step [22760/50782], Loss: 7.3442\n",
            "Epoch [1/10], Step [22770/50782], Loss: 8.7982\n",
            "Epoch [1/10], Step [22780/50782], Loss: 7.4390\n",
            "Epoch [1/10], Step [22790/50782], Loss: 7.5479\n",
            "Epoch [1/10], Step [22800/50782], Loss: 7.3660\n",
            "Epoch [1/10], Step [22810/50782], Loss: 9.1151\n",
            "Epoch [1/10], Step [22820/50782], Loss: 8.4011\n",
            "Epoch [1/10], Step [22830/50782], Loss: 7.6811\n",
            "Epoch [1/10], Step [22840/50782], Loss: 6.4688\n",
            "Epoch [1/10], Step [22850/50782], Loss: 6.6839\n",
            "Epoch [1/10], Step [22860/50782], Loss: 6.7274\n",
            "Epoch [1/10], Step [22870/50782], Loss: 7.2767\n",
            "Epoch [1/10], Step [22880/50782], Loss: 7.5455\n",
            "Epoch [1/10], Step [22890/50782], Loss: 6.6812\n",
            "Epoch [1/10], Step [22900/50782], Loss: 8.9783\n",
            "Epoch [1/10], Step [22910/50782], Loss: 6.3929\n",
            "Epoch [1/10], Step [22920/50782], Loss: 6.8109\n",
            "Epoch [1/10], Step [22930/50782], Loss: 8.3146\n",
            "Epoch [1/10], Step [22940/50782], Loss: 9.1170\n",
            "Epoch [1/10], Step [22950/50782], Loss: 8.5179\n",
            "Epoch [1/10], Step [22960/50782], Loss: 8.0013\n",
            "Epoch [1/10], Step [22970/50782], Loss: 5.9645\n",
            "Epoch [1/10], Step [22980/50782], Loss: 8.9220\n",
            "Epoch [1/10], Step [22990/50782], Loss: 8.0209\n",
            "Epoch [1/10], Step [23000/50782], Loss: 7.5872\n",
            "Epoch [1/10], Step [23010/50782], Loss: 7.6953\n",
            "Epoch [1/10], Step [23020/50782], Loss: 7.0685\n",
            "Epoch [1/10], Step [23030/50782], Loss: 7.8316\n",
            "Epoch [1/10], Step [23040/50782], Loss: 7.1065\n",
            "Epoch [1/10], Step [23050/50782], Loss: 8.3941\n",
            "Epoch [1/10], Step [23060/50782], Loss: 5.7852\n",
            "Epoch [1/10], Step [23070/50782], Loss: 7.5674\n",
            "Epoch [1/10], Step [23080/50782], Loss: 8.8370\n",
            "Epoch [1/10], Step [23090/50782], Loss: 8.2967\n",
            "Epoch [1/10], Step [23100/50782], Loss: 8.2071\n",
            "Epoch [1/10], Step [23110/50782], Loss: 7.4854\n",
            "Epoch [1/10], Step [23120/50782], Loss: 8.3954\n",
            "Epoch [1/10], Step [23130/50782], Loss: 7.4297\n",
            "Epoch [1/10], Step [23140/50782], Loss: 8.2036\n",
            "Epoch [1/10], Step [23150/50782], Loss: 8.4870\n",
            "Epoch [1/10], Step [23160/50782], Loss: 8.0715\n",
            "Epoch [1/10], Step [23170/50782], Loss: 7.3987\n",
            "Epoch [1/10], Step [23180/50782], Loss: 5.8380\n",
            "Epoch [1/10], Step [23190/50782], Loss: 6.9312\n",
            "Epoch [1/10], Step [23200/50782], Loss: 7.0815\n",
            "Epoch [1/10], Step [23210/50782], Loss: 7.6168\n",
            "Epoch [1/10], Step [23220/50782], Loss: 10.0033\n",
            "Epoch [1/10], Step [23230/50782], Loss: 6.8033\n",
            "Epoch [1/10], Step [23240/50782], Loss: 7.3073\n",
            "Epoch [1/10], Step [23250/50782], Loss: 7.8055\n",
            "Epoch [1/10], Step [23260/50782], Loss: 8.2544\n",
            "Epoch [1/10], Step [23270/50782], Loss: 9.1180\n",
            "Epoch [1/10], Step [23280/50782], Loss: 6.7138\n",
            "Epoch [1/10], Step [23290/50782], Loss: 6.8127\n",
            "Epoch [1/10], Step [23300/50782], Loss: 8.9856\n",
            "Epoch [1/10], Step [23310/50782], Loss: 6.1274\n",
            "Epoch [1/10], Step [23320/50782], Loss: 7.8987\n",
            "Epoch [1/10], Step [23330/50782], Loss: 7.5002\n",
            "Epoch [1/10], Step [23340/50782], Loss: 7.8969\n",
            "Epoch [1/10], Step [23350/50782], Loss: 6.5126\n",
            "Epoch [1/10], Step [23360/50782], Loss: 6.6125\n",
            "Epoch [1/10], Step [23370/50782], Loss: 8.4563\n",
            "Epoch [1/10], Step [23380/50782], Loss: 5.2665\n",
            "Epoch [1/10], Step [23390/50782], Loss: 8.2098\n",
            "Epoch [1/10], Step [23400/50782], Loss: 6.0051\n",
            "Epoch [1/10], Step [23410/50782], Loss: 7.9602\n",
            "Epoch [1/10], Step [23420/50782], Loss: 7.2953\n",
            "Epoch [1/10], Step [23430/50782], Loss: 8.3195\n",
            "Epoch [1/10], Step [23440/50782], Loss: 7.3808\n",
            "Epoch [1/10], Step [23450/50782], Loss: 7.3116\n",
            "Epoch [1/10], Step [23460/50782], Loss: 7.9681\n",
            "Epoch [1/10], Step [23470/50782], Loss: 7.2702\n",
            "Epoch [1/10], Step [23480/50782], Loss: 7.1786\n",
            "Epoch [1/10], Step [23490/50782], Loss: 7.9865\n",
            "Epoch [1/10], Step [23500/50782], Loss: 7.6769\n",
            "Epoch [1/10], Step [23510/50782], Loss: 8.3718\n",
            "Epoch [1/10], Step [23520/50782], Loss: 8.2238\n",
            "Epoch [1/10], Step [23530/50782], Loss: 7.1900\n",
            "Epoch [1/10], Step [23540/50782], Loss: 9.1070\n",
            "Epoch [1/10], Step [23550/50782], Loss: 6.7607\n",
            "Epoch [1/10], Step [23560/50782], Loss: 7.2314\n",
            "Epoch [1/10], Step [23570/50782], Loss: 8.2076\n",
            "Epoch [1/10], Step [23580/50782], Loss: 6.8203\n",
            "Epoch [1/10], Step [23590/50782], Loss: 7.8591\n",
            "Epoch [1/10], Step [23600/50782], Loss: 7.0033\n",
            "Epoch [1/10], Step [23610/50782], Loss: 8.2916\n",
            "Epoch [1/10], Step [23620/50782], Loss: 7.6322\n",
            "Epoch [1/10], Step [23630/50782], Loss: 8.3724\n",
            "Epoch [1/10], Step [23640/50782], Loss: 7.2380\n",
            "Epoch [1/10], Step [23650/50782], Loss: 6.8746\n",
            "Epoch [1/10], Step [23660/50782], Loss: 7.7230\n",
            "Epoch [1/10], Step [23670/50782], Loss: 6.3138\n",
            "Epoch [1/10], Step [23680/50782], Loss: 6.5282\n",
            "Epoch [1/10], Step [23690/50782], Loss: 7.2966\n",
            "Epoch [1/10], Step [23700/50782], Loss: 7.6618\n",
            "Epoch [1/10], Step [23710/50782], Loss: 5.4087\n",
            "Epoch [1/10], Step [23720/50782], Loss: 6.8837\n",
            "Epoch [1/10], Step [23730/50782], Loss: 7.3442\n",
            "Epoch [1/10], Step [23740/50782], Loss: 8.8511\n",
            "Epoch [1/10], Step [23750/50782], Loss: 9.0769\n",
            "Epoch [1/10], Step [23760/50782], Loss: 6.4606\n",
            "Epoch [1/10], Step [23770/50782], Loss: 6.9902\n",
            "Epoch [1/10], Step [23780/50782], Loss: 7.0621\n",
            "Epoch [1/10], Step [23790/50782], Loss: 7.3406\n",
            "Epoch [1/10], Step [23800/50782], Loss: 7.3365\n",
            "Epoch [1/10], Step [23810/50782], Loss: 6.1619\n",
            "Epoch [1/10], Step [23820/50782], Loss: 5.9192\n",
            "Epoch [1/10], Step [23830/50782], Loss: 7.1242\n",
            "Epoch [1/10], Step [23840/50782], Loss: 5.7736\n",
            "Epoch [1/10], Step [23850/50782], Loss: 7.8527\n",
            "Epoch [1/10], Step [23860/50782], Loss: 6.3618\n",
            "Epoch [1/10], Step [23870/50782], Loss: 7.9824\n",
            "Epoch [1/10], Step [23880/50782], Loss: 6.9226\n",
            "Epoch [1/10], Step [23890/50782], Loss: 6.4515\n",
            "Epoch [1/10], Step [23900/50782], Loss: 6.3297\n",
            "Epoch [1/10], Step [23910/50782], Loss: 6.9918\n",
            "Epoch [1/10], Step [23920/50782], Loss: 8.5391\n",
            "Epoch [1/10], Step [23930/50782], Loss: 8.0155\n",
            "Epoch [1/10], Step [23940/50782], Loss: 6.4321\n",
            "Epoch [1/10], Step [23950/50782], Loss: 7.2373\n",
            "Epoch [1/10], Step [23960/50782], Loss: 8.7959\n",
            "Epoch [1/10], Step [23970/50782], Loss: 9.7452\n",
            "Epoch [1/10], Step [23980/50782], Loss: 7.1656\n",
            "Epoch [1/10], Step [23990/50782], Loss: 6.2932\n",
            "Epoch [1/10], Step [24000/50782], Loss: 6.5649\n",
            "Epoch [1/10], Step [24010/50782], Loss: 6.6814\n",
            "Epoch [1/10], Step [24020/50782], Loss: 6.1429\n",
            "Epoch [1/10], Step [24030/50782], Loss: 7.7058\n",
            "Epoch [1/10], Step [24040/50782], Loss: 8.5916\n",
            "Epoch [1/10], Step [24050/50782], Loss: 7.3259\n",
            "Epoch [1/10], Step [24060/50782], Loss: 7.0827\n",
            "Epoch [1/10], Step [24070/50782], Loss: 7.1770\n",
            "Epoch [1/10], Step [24080/50782], Loss: 6.0988\n",
            "Epoch [1/10], Step [24090/50782], Loss: 9.8876\n",
            "Epoch [1/10], Step [24100/50782], Loss: 5.9336\n",
            "Epoch [1/10], Step [24110/50782], Loss: 7.3632\n",
            "Epoch [1/10], Step [24120/50782], Loss: 6.9535\n",
            "Epoch [1/10], Step [24130/50782], Loss: 7.0063\n",
            "Epoch [1/10], Step [24140/50782], Loss: 7.7454\n",
            "Epoch [1/10], Step [24150/50782], Loss: 7.2784\n",
            "Epoch [1/10], Step [24160/50782], Loss: 6.8390\n",
            "Epoch [1/10], Step [24170/50782], Loss: 6.8369\n",
            "Epoch [1/10], Step [24180/50782], Loss: 8.0301\n",
            "Epoch [1/10], Step [24190/50782], Loss: 7.5357\n",
            "Epoch [1/10], Step [24200/50782], Loss: 7.1678\n",
            "Epoch [1/10], Step [24210/50782], Loss: 8.4423\n",
            "Epoch [1/10], Step [24220/50782], Loss: 6.9356\n",
            "Epoch [1/10], Step [24230/50782], Loss: 6.5689\n",
            "Epoch [1/10], Step [24240/50782], Loss: 7.8135\n",
            "Epoch [1/10], Step [24250/50782], Loss: 7.4155\n",
            "Epoch [1/10], Step [24260/50782], Loss: 6.5038\n",
            "Epoch [1/10], Step [24270/50782], Loss: 6.9120\n",
            "Epoch [1/10], Step [24280/50782], Loss: 7.6552\n",
            "Epoch [1/10], Step [24290/50782], Loss: 7.6706\n",
            "Epoch [1/10], Step [24300/50782], Loss: 7.8338\n",
            "Epoch [1/10], Step [24310/50782], Loss: 8.0172\n",
            "Epoch [1/10], Step [24320/50782], Loss: 6.0235\n",
            "Epoch [1/10], Step [24330/50782], Loss: 6.5050\n",
            "Epoch [1/10], Step [24340/50782], Loss: 7.7404\n",
            "Epoch [1/10], Step [24350/50782], Loss: 6.9954\n",
            "Epoch [1/10], Step [24360/50782], Loss: 8.5915\n",
            "Epoch [1/10], Step [24370/50782], Loss: 7.6659\n",
            "Epoch [1/10], Step [24380/50782], Loss: 6.4955\n",
            "Epoch [1/10], Step [24390/50782], Loss: 8.0540\n",
            "Epoch [1/10], Step [24400/50782], Loss: 8.1632\n",
            "Epoch [1/10], Step [24410/50782], Loss: 6.2092\n",
            "Epoch [1/10], Step [24420/50782], Loss: 6.2890\n",
            "Epoch [1/10], Step [24430/50782], Loss: 6.3044\n",
            "Epoch [1/10], Step [24440/50782], Loss: 8.5708\n",
            "Epoch [1/10], Step [24450/50782], Loss: 7.1261\n",
            "Epoch [1/10], Step [24460/50782], Loss: 9.4598\n",
            "Epoch [1/10], Step [24470/50782], Loss: 7.2855\n",
            "Epoch [1/10], Step [24480/50782], Loss: 7.1067\n",
            "Epoch [1/10], Step [24490/50782], Loss: 7.5674\n",
            "Epoch [1/10], Step [24500/50782], Loss: 6.0964\n",
            "Epoch [1/10], Step [24510/50782], Loss: 6.5187\n",
            "Epoch [1/10], Step [24520/50782], Loss: 7.7284\n",
            "Epoch [1/10], Step [24530/50782], Loss: 8.1544\n",
            "Epoch [1/10], Step [24540/50782], Loss: 6.8785\n",
            "Epoch [1/10], Step [24550/50782], Loss: 9.0584\n",
            "Epoch [1/10], Step [24560/50782], Loss: 8.3249\n",
            "Epoch [1/10], Step [24570/50782], Loss: 5.7375\n",
            "Epoch [1/10], Step [24580/50782], Loss: 5.7055\n",
            "Epoch [1/10], Step [24590/50782], Loss: 8.8773\n",
            "Epoch [1/10], Step [24600/50782], Loss: 8.0263\n",
            "Epoch [1/10], Step [24610/50782], Loss: 6.2290\n",
            "Epoch [1/10], Step [24620/50782], Loss: 8.1365\n",
            "Epoch [1/10], Step [24630/50782], Loss: 6.4709\n",
            "Epoch [1/10], Step [24640/50782], Loss: 7.9533\n",
            "Epoch [1/10], Step [24650/50782], Loss: 6.5821\n",
            "Epoch [1/10], Step [24660/50782], Loss: 10.0563\n",
            "Epoch [1/10], Step [24670/50782], Loss: 7.6250\n",
            "Epoch [1/10], Step [24680/50782], Loss: 6.4770\n",
            "Epoch [1/10], Step [24690/50782], Loss: 8.7293\n",
            "Epoch [1/10], Step [24700/50782], Loss: 7.2676\n",
            "Epoch [1/10], Step [24710/50782], Loss: 6.7602\n",
            "Epoch [1/10], Step [24720/50782], Loss: 8.4896\n",
            "Epoch [1/10], Step [24730/50782], Loss: 5.5621\n",
            "Epoch [1/10], Step [24740/50782], Loss: 6.2033\n",
            "Epoch [1/10], Step [24750/50782], Loss: 6.1955\n",
            "Epoch [1/10], Step [24760/50782], Loss: 7.1467\n",
            "Epoch [1/10], Step [24770/50782], Loss: 8.5939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"next_word_model.pth\")\n",
        "print(\"✅ Model trained & saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnTWJ3Sgt5w5",
        "outputId": "c1fd6537-1a08-4c7a-8457-cb478c9933e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model trained & saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load trained model\n",
        "model = NextWordModel(vocab_size)\n",
        "model.load_state_dict(torch.load(\"next_word_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Create reverse vocab mapping (index -> word)\n",
        "index_to_word = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "def predict_next_word(sentence):\n",
        "    words = sentence.split()\n",
        "    encoded = [vocab[word] for word in words if word in vocab]\n",
        "\n",
        "    # Ensure sequence length\n",
        "    if len(encoded) < seq_length:\n",
        "        encoded = [0] * (seq_length - len(encoded)) + encoded  # Padding if needed\n",
        "\n",
        "    # Convert to tensor\n",
        "    input_tensor = torch.tensor([encoded[-seq_length:]], dtype=torch.long)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_tensor)\n",
        "\n",
        "    # Get highest probability word index\n",
        "    predicted_index = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # Lookup word from index\n",
        "    predicted_word = index_to_word.get(predicted_index, \"UNKNOWN\")\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example Prediction\n",
        "print(predict_next_word(\"battle\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_de_jeDZt8On",
        "outputId": "fbd06482-1ba6-4d99-f805-3865e71a06f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prosperous\".\n"
          ]
        }
      ]
    }
  ]
}